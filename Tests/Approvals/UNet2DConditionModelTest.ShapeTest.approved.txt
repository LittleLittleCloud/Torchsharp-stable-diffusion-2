0: conv_in.bias shape: [320]
1: conv_in.weight shape: [320, 4, 3, 3]
2: conv_norm_out.bias shape: [320]
3: conv_norm_out.weight shape: [320]
4: conv_out.bias shape: [4]
5: conv_out.weight shape: [4, 320, 3, 3]
6: down_blocks.0.attentions.0.norm.bias shape: [320]
7: down_blocks.0.attentions.0.norm.weight shape: [320]
8: down_blocks.0.attentions.0.proj_in.bias shape: [320]
9: down_blocks.0.attentions.0.proj_in.weight shape: [320, 320]
10: down_blocks.0.attentions.0.proj_out.bias shape: [320]
11: down_blocks.0.attentions.0.proj_out.weight shape: [320, 320]
12: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight shape: [320, 320]
13: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias shape: [320]
14: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight shape: [320, 320]
15: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight shape: [320, 320]
16: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight shape: [320, 320]
17: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight shape: [320, 1024]
18: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias shape: [320]
19: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight shape: [320, 320]
20: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight shape: [320, 320]
21: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight shape: [320, 1024]
22: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias shape: [2560]
23: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight shape: [2560, 320]
24: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias shape: [320]
25: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight shape: [320, 1280]
26: down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias shape: [320]
27: down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight shape: [320]
28: down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias shape: [320]
29: down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight shape: [320]
30: down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias shape: [320]
31: down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight shape: [320]
32: down_blocks.0.attentions.1.norm.bias shape: [320]
33: down_blocks.0.attentions.1.norm.weight shape: [320]
34: down_blocks.0.attentions.1.proj_in.bias shape: [320]
35: down_blocks.0.attentions.1.proj_in.weight shape: [320, 320]
36: down_blocks.0.attentions.1.proj_out.bias shape: [320]
37: down_blocks.0.attentions.1.proj_out.weight shape: [320, 320]
38: down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight shape: [320, 320]
39: down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias shape: [320]
40: down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight shape: [320, 320]
41: down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight shape: [320, 320]
42: down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight shape: [320, 320]
43: down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight shape: [320, 1024]
44: down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias shape: [320]
45: down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight shape: [320, 320]
46: down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight shape: [320, 320]
47: down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight shape: [320, 1024]
48: down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias shape: [2560]
49: down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight shape: [2560, 320]
50: down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias shape: [320]
51: down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight shape: [320, 1280]
52: down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias shape: [320]
53: down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight shape: [320]
54: down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias shape: [320]
55: down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight shape: [320]
56: down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias shape: [320]
57: down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight shape: [320]
58: down_blocks.0.downsamplers.0.conv.bias shape: [320]
59: down_blocks.0.downsamplers.0.conv.weight shape: [320, 320, 3, 3]
60: down_blocks.0.resnets.0.conv1.bias shape: [320]
61: down_blocks.0.resnets.0.conv1.weight shape: [320, 320, 3, 3]
62: down_blocks.0.resnets.0.conv2.bias shape: [320]
63: down_blocks.0.resnets.0.conv2.weight shape: [320, 320, 3, 3]
64: down_blocks.0.resnets.0.norm1.bias shape: [320]
65: down_blocks.0.resnets.0.norm1.weight shape: [320]
66: down_blocks.0.resnets.0.norm2.bias shape: [320]
67: down_blocks.0.resnets.0.norm2.weight shape: [320]
68: down_blocks.0.resnets.0.time_emb_proj.bias shape: [320]
69: down_blocks.0.resnets.0.time_emb_proj.weight shape: [320, 1280]
70: down_blocks.0.resnets.1.conv1.bias shape: [320]
71: down_blocks.0.resnets.1.conv1.weight shape: [320, 320, 3, 3]
72: down_blocks.0.resnets.1.conv2.bias shape: [320]
73: down_blocks.0.resnets.1.conv2.weight shape: [320, 320, 3, 3]
74: down_blocks.0.resnets.1.norm1.bias shape: [320]
75: down_blocks.0.resnets.1.norm1.weight shape: [320]
76: down_blocks.0.resnets.1.norm2.bias shape: [320]
77: down_blocks.0.resnets.1.norm2.weight shape: [320]
78: down_blocks.0.resnets.1.time_emb_proj.bias shape: [320]
79: down_blocks.0.resnets.1.time_emb_proj.weight shape: [320, 1280]
80: down_blocks.1.attentions.0.norm.bias shape: [640]
81: down_blocks.1.attentions.0.norm.weight shape: [640]
82: down_blocks.1.attentions.0.proj_in.bias shape: [640]
83: down_blocks.1.attentions.0.proj_in.weight shape: [640, 640]
84: down_blocks.1.attentions.0.proj_out.bias shape: [640]
85: down_blocks.1.attentions.0.proj_out.weight shape: [640, 640]
86: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight shape: [640, 640]
87: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias shape: [640]
88: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight shape: [640, 640]
89: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight shape: [640, 640]
90: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight shape: [640, 640]
91: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight shape: [640, 1024]
92: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias shape: [640]
93: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight shape: [640, 640]
94: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight shape: [640, 640]
95: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight shape: [640, 1024]
96: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias shape: [5120]
97: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight shape: [5120, 640]
98: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias shape: [640]
99: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight shape: [640, 2560]
100: down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias shape: [640]
101: down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight shape: [640]
102: down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias shape: [640]
103: down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight shape: [640]
104: down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias shape: [640]
105: down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight shape: [640]
106: down_blocks.1.attentions.1.norm.bias shape: [640]
107: down_blocks.1.attentions.1.norm.weight shape: [640]
108: down_blocks.1.attentions.1.proj_in.bias shape: [640]
109: down_blocks.1.attentions.1.proj_in.weight shape: [640, 640]
110: down_blocks.1.attentions.1.proj_out.bias shape: [640]
111: down_blocks.1.attentions.1.proj_out.weight shape: [640, 640]
112: down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight shape: [640, 640]
113: down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias shape: [640]
114: down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight shape: [640, 640]
115: down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight shape: [640, 640]
116: down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight shape: [640, 640]
117: down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight shape: [640, 1024]
118: down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias shape: [640]
119: down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight shape: [640, 640]
120: down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight shape: [640, 640]
121: down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight shape: [640, 1024]
122: down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias shape: [5120]
123: down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight shape: [5120, 640]
124: down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias shape: [640]
125: down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight shape: [640, 2560]
126: down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias shape: [640]
127: down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight shape: [640]
128: down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias shape: [640]
129: down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight shape: [640]
130: down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias shape: [640]
131: down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight shape: [640]
132: down_blocks.1.downsamplers.0.conv.bias shape: [640]
133: down_blocks.1.downsamplers.0.conv.weight shape: [640, 640, 3, 3]
134: down_blocks.1.resnets.0.conv1.bias shape: [640]
135: down_blocks.1.resnets.0.conv1.weight shape: [640, 320, 3, 3]
136: down_blocks.1.resnets.0.conv2.bias shape: [640]
137: down_blocks.1.resnets.0.conv2.weight shape: [640, 640, 3, 3]
138: down_blocks.1.resnets.0.conv_shortcut.bias shape: [640]
139: down_blocks.1.resnets.0.conv_shortcut.weight shape: [640, 320, 1, 1]
140: down_blocks.1.resnets.0.norm1.bias shape: [320]
141: down_blocks.1.resnets.0.norm1.weight shape: [320]
142: down_blocks.1.resnets.0.norm2.bias shape: [640]
143: down_blocks.1.resnets.0.norm2.weight shape: [640]
144: down_blocks.1.resnets.0.time_emb_proj.bias shape: [640]
145: down_blocks.1.resnets.0.time_emb_proj.weight shape: [640, 1280]
146: down_blocks.1.resnets.1.conv1.bias shape: [640]
147: down_blocks.1.resnets.1.conv1.weight shape: [640, 640, 3, 3]
148: down_blocks.1.resnets.1.conv2.bias shape: [640]
149: down_blocks.1.resnets.1.conv2.weight shape: [640, 640, 3, 3]
150: down_blocks.1.resnets.1.norm1.bias shape: [640]
151: down_blocks.1.resnets.1.norm1.weight shape: [640]
152: down_blocks.1.resnets.1.norm2.bias shape: [640]
153: down_blocks.1.resnets.1.norm2.weight shape: [640]
154: down_blocks.1.resnets.1.time_emb_proj.bias shape: [640]
155: down_blocks.1.resnets.1.time_emb_proj.weight shape: [640, 1280]
156: down_blocks.2.attentions.0.norm.bias shape: [1280]
157: down_blocks.2.attentions.0.norm.weight shape: [1280]
158: down_blocks.2.attentions.0.proj_in.bias shape: [1280]
159: down_blocks.2.attentions.0.proj_in.weight shape: [1280, 1280]
160: down_blocks.2.attentions.0.proj_out.bias shape: [1280]
161: down_blocks.2.attentions.0.proj_out.weight shape: [1280, 1280]
162: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight shape: [1280, 1280]
163: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias shape: [1280]
164: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight shape: [1280, 1280]
165: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight shape: [1280, 1280]
166: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight shape: [1280, 1280]
167: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight shape: [1280, 1024]
168: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias shape: [1280]
169: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight shape: [1280, 1280]
170: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight shape: [1280, 1280]
171: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight shape: [1280, 1024]
172: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias shape: [10240]
173: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight shape: [10240, 1280]
174: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias shape: [1280]
175: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight shape: [1280, 5120]
176: down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias shape: [1280]
177: down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight shape: [1280]
178: down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias shape: [1280]
179: down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight shape: [1280]
180: down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias shape: [1280]
181: down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight shape: [1280]
182: down_blocks.2.attentions.1.norm.bias shape: [1280]
183: down_blocks.2.attentions.1.norm.weight shape: [1280]
184: down_blocks.2.attentions.1.proj_in.bias shape: [1280]
185: down_blocks.2.attentions.1.proj_in.weight shape: [1280, 1280]
186: down_blocks.2.attentions.1.proj_out.bias shape: [1280]
187: down_blocks.2.attentions.1.proj_out.weight shape: [1280, 1280]
188: down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight shape: [1280, 1280]
189: down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias shape: [1280]
190: down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight shape: [1280, 1280]
191: down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight shape: [1280, 1280]
192: down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight shape: [1280, 1280]
193: down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight shape: [1280, 1024]
194: down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias shape: [1280]
195: down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight shape: [1280, 1280]
196: down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight shape: [1280, 1280]
197: down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight shape: [1280, 1024]
198: down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias shape: [10240]
199: down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight shape: [10240, 1280]
200: down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias shape: [1280]
201: down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight shape: [1280, 5120]
202: down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias shape: [1280]
203: down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight shape: [1280]
204: down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias shape: [1280]
205: down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight shape: [1280]
206: down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias shape: [1280]
207: down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight shape: [1280]
208: down_blocks.2.downsamplers.0.conv.bias shape: [1280]
209: down_blocks.2.downsamplers.0.conv.weight shape: [1280, 1280, 3, 3]
210: down_blocks.2.resnets.0.conv1.bias shape: [1280]
211: down_blocks.2.resnets.0.conv1.weight shape: [1280, 640, 3, 3]
212: down_blocks.2.resnets.0.conv2.bias shape: [1280]
213: down_blocks.2.resnets.0.conv2.weight shape: [1280, 1280, 3, 3]
214: down_blocks.2.resnets.0.conv_shortcut.bias shape: [1280]
215: down_blocks.2.resnets.0.conv_shortcut.weight shape: [1280, 640, 1, 1]
216: down_blocks.2.resnets.0.norm1.bias shape: [640]
217: down_blocks.2.resnets.0.norm1.weight shape: [640]
218: down_blocks.2.resnets.0.norm2.bias shape: [1280]
219: down_blocks.2.resnets.0.norm2.weight shape: [1280]
220: down_blocks.2.resnets.0.time_emb_proj.bias shape: [1280]
221: down_blocks.2.resnets.0.time_emb_proj.weight shape: [1280, 1280]
222: down_blocks.2.resnets.1.conv1.bias shape: [1280]
223: down_blocks.2.resnets.1.conv1.weight shape: [1280, 1280, 3, 3]
224: down_blocks.2.resnets.1.conv2.bias shape: [1280]
225: down_blocks.2.resnets.1.conv2.weight shape: [1280, 1280, 3, 3]
226: down_blocks.2.resnets.1.norm1.bias shape: [1280]
227: down_blocks.2.resnets.1.norm1.weight shape: [1280]
228: down_blocks.2.resnets.1.norm2.bias shape: [1280]
229: down_blocks.2.resnets.1.norm2.weight shape: [1280]
230: down_blocks.2.resnets.1.time_emb_proj.bias shape: [1280]
231: down_blocks.2.resnets.1.time_emb_proj.weight shape: [1280, 1280]
232: down_blocks.3.resnets.0.conv1.bias shape: [1280]
233: down_blocks.3.resnets.0.conv1.weight shape: [1280, 1280, 3, 3]
234: down_blocks.3.resnets.0.conv2.bias shape: [1280]
235: down_blocks.3.resnets.0.conv2.weight shape: [1280, 1280, 3, 3]
236: down_blocks.3.resnets.0.norm1.bias shape: [1280]
237: down_blocks.3.resnets.0.norm1.weight shape: [1280]
238: down_blocks.3.resnets.0.norm2.bias shape: [1280]
239: down_blocks.3.resnets.0.norm2.weight shape: [1280]
240: down_blocks.3.resnets.0.time_emb_proj.bias shape: [1280]
241: down_blocks.3.resnets.0.time_emb_proj.weight shape: [1280, 1280]
242: down_blocks.3.resnets.1.conv1.bias shape: [1280]
243: down_blocks.3.resnets.1.conv1.weight shape: [1280, 1280, 3, 3]
244: down_blocks.3.resnets.1.conv2.bias shape: [1280]
245: down_blocks.3.resnets.1.conv2.weight shape: [1280, 1280, 3, 3]
246: down_blocks.3.resnets.1.norm1.bias shape: [1280]
247: down_blocks.3.resnets.1.norm1.weight shape: [1280]
248: down_blocks.3.resnets.1.norm2.bias shape: [1280]
249: down_blocks.3.resnets.1.norm2.weight shape: [1280]
250: down_blocks.3.resnets.1.time_emb_proj.bias shape: [1280]
251: down_blocks.3.resnets.1.time_emb_proj.weight shape: [1280, 1280]
252: mid_block.attentions.0.norm.bias shape: [1280]
253: mid_block.attentions.0.norm.weight shape: [1280]
254: mid_block.attentions.0.proj_in.bias shape: [1280]
255: mid_block.attentions.0.proj_in.weight shape: [1280, 1280]
256: mid_block.attentions.0.proj_out.bias shape: [1280]
257: mid_block.attentions.0.proj_out.weight shape: [1280, 1280]
258: mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight shape: [1280, 1280]
259: mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias shape: [1280]
260: mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight shape: [1280, 1280]
261: mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight shape: [1280, 1280]
262: mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight shape: [1280, 1280]
263: mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight shape: [1280, 1024]
264: mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias shape: [1280]
265: mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight shape: [1280, 1280]
266: mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight shape: [1280, 1280]
267: mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight shape: [1280, 1024]
268: mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias shape: [10240]
269: mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight shape: [10240, 1280]
270: mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias shape: [1280]
271: mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight shape: [1280, 5120]
272: mid_block.attentions.0.transformer_blocks.0.norm1.bias shape: [1280]
273: mid_block.attentions.0.transformer_blocks.0.norm1.weight shape: [1280]
274: mid_block.attentions.0.transformer_blocks.0.norm2.bias shape: [1280]
275: mid_block.attentions.0.transformer_blocks.0.norm2.weight shape: [1280]
276: mid_block.attentions.0.transformer_blocks.0.norm3.bias shape: [1280]
277: mid_block.attentions.0.transformer_blocks.0.norm3.weight shape: [1280]
278: mid_block.resnets.0.conv1.bias shape: [1280]
279: mid_block.resnets.0.conv1.weight shape: [1280, 1280, 3, 3]
280: mid_block.resnets.0.conv2.bias shape: [1280]
281: mid_block.resnets.0.conv2.weight shape: [1280, 1280, 3, 3]
282: mid_block.resnets.0.norm1.bias shape: [1280]
283: mid_block.resnets.0.norm1.weight shape: [1280]
284: mid_block.resnets.0.norm2.bias shape: [1280]
285: mid_block.resnets.0.norm2.weight shape: [1280]
286: mid_block.resnets.0.time_emb_proj.bias shape: [1280]
287: mid_block.resnets.0.time_emb_proj.weight shape: [1280, 1280]
288: mid_block.resnets.1.conv1.bias shape: [1280]
289: mid_block.resnets.1.conv1.weight shape: [1280, 1280, 3, 3]
290: mid_block.resnets.1.conv2.bias shape: [1280]
291: mid_block.resnets.1.conv2.weight shape: [1280, 1280, 3, 3]
292: mid_block.resnets.1.norm1.bias shape: [1280]
293: mid_block.resnets.1.norm1.weight shape: [1280]
294: mid_block.resnets.1.norm2.bias shape: [1280]
295: mid_block.resnets.1.norm2.weight shape: [1280]
296: mid_block.resnets.1.time_emb_proj.bias shape: [1280]
297: mid_block.resnets.1.time_emb_proj.weight shape: [1280, 1280]
298: time_embedding.linear_1.bias shape: [1280]
299: time_embedding.linear_1.weight shape: [1280, 320]
300: time_embedding.linear_2.bias shape: [1280]
301: time_embedding.linear_2.weight shape: [1280, 1280]
302: up_blocks.0.resnets.0.conv1.bias shape: [1280]
303: up_blocks.0.resnets.0.conv1.weight shape: [1280, 2560, 3, 3]
304: up_blocks.0.resnets.0.conv2.bias shape: [1280]
305: up_blocks.0.resnets.0.conv2.weight shape: [1280, 1280, 3, 3]
306: up_blocks.0.resnets.0.conv_shortcut.bias shape: [1280]
307: up_blocks.0.resnets.0.conv_shortcut.weight shape: [1280, 2560, 1, 1]
308: up_blocks.0.resnets.0.norm1.bias shape: [2560]
309: up_blocks.0.resnets.0.norm1.weight shape: [2560]
310: up_blocks.0.resnets.0.norm2.bias shape: [1280]
311: up_blocks.0.resnets.0.norm2.weight shape: [1280]
312: up_blocks.0.resnets.0.time_emb_proj.bias shape: [1280]
313: up_blocks.0.resnets.0.time_emb_proj.weight shape: [1280, 1280]
314: up_blocks.0.resnets.1.conv1.bias shape: [1280]
315: up_blocks.0.resnets.1.conv1.weight shape: [1280, 2560, 3, 3]
316: up_blocks.0.resnets.1.conv2.bias shape: [1280]
317: up_blocks.0.resnets.1.conv2.weight shape: [1280, 1280, 3, 3]
318: up_blocks.0.resnets.1.conv_shortcut.bias shape: [1280]
319: up_blocks.0.resnets.1.conv_shortcut.weight shape: [1280, 2560, 1, 1]
320: up_blocks.0.resnets.1.norm1.bias shape: [2560]
321: up_blocks.0.resnets.1.norm1.weight shape: [2560]
322: up_blocks.0.resnets.1.norm2.bias shape: [1280]
323: up_blocks.0.resnets.1.norm2.weight shape: [1280]
324: up_blocks.0.resnets.1.time_emb_proj.bias shape: [1280]
325: up_blocks.0.resnets.1.time_emb_proj.weight shape: [1280, 1280]
326: up_blocks.0.resnets.2.conv1.bias shape: [1280]
327: up_blocks.0.resnets.2.conv1.weight shape: [1280, 2560, 3, 3]
328: up_blocks.0.resnets.2.conv2.bias shape: [1280]
329: up_blocks.0.resnets.2.conv2.weight shape: [1280, 1280, 3, 3]
330: up_blocks.0.resnets.2.conv_shortcut.bias shape: [1280]
331: up_blocks.0.resnets.2.conv_shortcut.weight shape: [1280, 2560, 1, 1]
332: up_blocks.0.resnets.2.norm1.bias shape: [2560]
333: up_blocks.0.resnets.2.norm1.weight shape: [2560]
334: up_blocks.0.resnets.2.norm2.bias shape: [1280]
335: up_blocks.0.resnets.2.norm2.weight shape: [1280]
336: up_blocks.0.resnets.2.time_emb_proj.bias shape: [1280]
337: up_blocks.0.resnets.2.time_emb_proj.weight shape: [1280, 1280]
338: up_blocks.0.upsamplers.0.conv.bias shape: [1280]
339: up_blocks.0.upsamplers.0.conv.weight shape: [1280, 1280, 3, 3]
340: up_blocks.1.attentions.0.norm.bias shape: [1280]
341: up_blocks.1.attentions.0.norm.weight shape: [1280]
342: up_blocks.1.attentions.0.proj_in.bias shape: [1280]
343: up_blocks.1.attentions.0.proj_in.weight shape: [1280, 1280]
344: up_blocks.1.attentions.0.proj_out.bias shape: [1280]
345: up_blocks.1.attentions.0.proj_out.weight shape: [1280, 1280]
346: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight shape: [1280, 1280]
347: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias shape: [1280]
348: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight shape: [1280, 1280]
349: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight shape: [1280, 1280]
350: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight shape: [1280, 1280]
351: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight shape: [1280, 1024]
352: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias shape: [1280]
353: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight shape: [1280, 1280]
354: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight shape: [1280, 1280]
355: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight shape: [1280, 1024]
356: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias shape: [10240]
357: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight shape: [10240, 1280]
358: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias shape: [1280]
359: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight shape: [1280, 5120]
360: up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias shape: [1280]
361: up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight shape: [1280]
362: up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias shape: [1280]
363: up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight shape: [1280]
364: up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias shape: [1280]
365: up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight shape: [1280]
366: up_blocks.1.attentions.1.norm.bias shape: [1280]
367: up_blocks.1.attentions.1.norm.weight shape: [1280]
368: up_blocks.1.attentions.1.proj_in.bias shape: [1280]
369: up_blocks.1.attentions.1.proj_in.weight shape: [1280, 1280]
370: up_blocks.1.attentions.1.proj_out.bias shape: [1280]
371: up_blocks.1.attentions.1.proj_out.weight shape: [1280, 1280]
372: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight shape: [1280, 1280]
373: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias shape: [1280]
374: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight shape: [1280, 1280]
375: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight shape: [1280, 1280]
376: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight shape: [1280, 1280]
377: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight shape: [1280, 1024]
378: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias shape: [1280]
379: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight shape: [1280, 1280]
380: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight shape: [1280, 1280]
381: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight shape: [1280, 1024]
382: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias shape: [10240]
383: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight shape: [10240, 1280]
384: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias shape: [1280]
385: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight shape: [1280, 5120]
386: up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias shape: [1280]
387: up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight shape: [1280]
388: up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias shape: [1280]
389: up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight shape: [1280]
390: up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias shape: [1280]
391: up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight shape: [1280]
392: up_blocks.1.attentions.2.norm.bias shape: [1280]
393: up_blocks.1.attentions.2.norm.weight shape: [1280]
394: up_blocks.1.attentions.2.proj_in.bias shape: [1280]
395: up_blocks.1.attentions.2.proj_in.weight shape: [1280, 1280]
396: up_blocks.1.attentions.2.proj_out.bias shape: [1280]
397: up_blocks.1.attentions.2.proj_out.weight shape: [1280, 1280]
398: up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight shape: [1280, 1280]
399: up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias shape: [1280]
400: up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight shape: [1280, 1280]
401: up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight shape: [1280, 1280]
402: up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight shape: [1280, 1280]
403: up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight shape: [1280, 1024]
404: up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias shape: [1280]
405: up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight shape: [1280, 1280]
406: up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight shape: [1280, 1280]
407: up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight shape: [1280, 1024]
408: up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias shape: [10240]
409: up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight shape: [10240, 1280]
410: up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias shape: [1280]
411: up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight shape: [1280, 5120]
412: up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias shape: [1280]
413: up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight shape: [1280]
414: up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias shape: [1280]
415: up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight shape: [1280]
416: up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias shape: [1280]
417: up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight shape: [1280]
418: up_blocks.1.resnets.0.conv1.bias shape: [1280]
419: up_blocks.1.resnets.0.conv1.weight shape: [1280, 2560, 3, 3]
420: up_blocks.1.resnets.0.conv2.bias shape: [1280]
421: up_blocks.1.resnets.0.conv2.weight shape: [1280, 1280, 3, 3]
422: up_blocks.1.resnets.0.conv_shortcut.bias shape: [1280]
423: up_blocks.1.resnets.0.conv_shortcut.weight shape: [1280, 2560, 1, 1]
424: up_blocks.1.resnets.0.norm1.bias shape: [2560]
425: up_blocks.1.resnets.0.norm1.weight shape: [2560]
426: up_blocks.1.resnets.0.norm2.bias shape: [1280]
427: up_blocks.1.resnets.0.norm2.weight shape: [1280]
428: up_blocks.1.resnets.0.time_emb_proj.bias shape: [1280]
429: up_blocks.1.resnets.0.time_emb_proj.weight shape: [1280, 1280]
430: up_blocks.1.resnets.1.conv1.bias shape: [1280]
431: up_blocks.1.resnets.1.conv1.weight shape: [1280, 2560, 3, 3]
432: up_blocks.1.resnets.1.conv2.bias shape: [1280]
433: up_blocks.1.resnets.1.conv2.weight shape: [1280, 1280, 3, 3]
434: up_blocks.1.resnets.1.conv_shortcut.bias shape: [1280]
435: up_blocks.1.resnets.1.conv_shortcut.weight shape: [1280, 2560, 1, 1]
436: up_blocks.1.resnets.1.norm1.bias shape: [2560]
437: up_blocks.1.resnets.1.norm1.weight shape: [2560]
438: up_blocks.1.resnets.1.norm2.bias shape: [1280]
439: up_blocks.1.resnets.1.norm2.weight shape: [1280]
440: up_blocks.1.resnets.1.time_emb_proj.bias shape: [1280]
441: up_blocks.1.resnets.1.time_emb_proj.weight shape: [1280, 1280]
442: up_blocks.1.resnets.2.conv1.bias shape: [1280]
443: up_blocks.1.resnets.2.conv1.weight shape: [1280, 1920, 3, 3]
444: up_blocks.1.resnets.2.conv2.bias shape: [1280]
445: up_blocks.1.resnets.2.conv2.weight shape: [1280, 1280, 3, 3]
446: up_blocks.1.resnets.2.conv_shortcut.bias shape: [1280]
447: up_blocks.1.resnets.2.conv_shortcut.weight shape: [1280, 1920, 1, 1]
448: up_blocks.1.resnets.2.norm1.bias shape: [1920]
449: up_blocks.1.resnets.2.norm1.weight shape: [1920]
450: up_blocks.1.resnets.2.norm2.bias shape: [1280]
451: up_blocks.1.resnets.2.norm2.weight shape: [1280]
452: up_blocks.1.resnets.2.time_emb_proj.bias shape: [1280]
453: up_blocks.1.resnets.2.time_emb_proj.weight shape: [1280, 1280]
454: up_blocks.1.upsamplers.0.conv.bias shape: [1280]
455: up_blocks.1.upsamplers.0.conv.weight shape: [1280, 1280, 3, 3]
456: up_blocks.2.attentions.0.norm.bias shape: [640]
457: up_blocks.2.attentions.0.norm.weight shape: [640]
458: up_blocks.2.attentions.0.proj_in.bias shape: [640]
459: up_blocks.2.attentions.0.proj_in.weight shape: [640, 640]
460: up_blocks.2.attentions.0.proj_out.bias shape: [640]
461: up_blocks.2.attentions.0.proj_out.weight shape: [640, 640]
462: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight shape: [640, 640]
463: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias shape: [640]
464: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight shape: [640, 640]
465: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight shape: [640, 640]
466: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight shape: [640, 640]
467: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight shape: [640, 1024]
468: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias shape: [640]
469: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight shape: [640, 640]
470: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight shape: [640, 640]
471: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight shape: [640, 1024]
472: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias shape: [5120]
473: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight shape: [5120, 640]
474: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias shape: [640]
475: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight shape: [640, 2560]
476: up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias shape: [640]
477: up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight shape: [640]
478: up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias shape: [640]
479: up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight shape: [640]
480: up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias shape: [640]
481: up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight shape: [640]
482: up_blocks.2.attentions.1.norm.bias shape: [640]
483: up_blocks.2.attentions.1.norm.weight shape: [640]
484: up_blocks.2.attentions.1.proj_in.bias shape: [640]
485: up_blocks.2.attentions.1.proj_in.weight shape: [640, 640]
486: up_blocks.2.attentions.1.proj_out.bias shape: [640]
487: up_blocks.2.attentions.1.proj_out.weight shape: [640, 640]
488: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight shape: [640, 640]
489: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias shape: [640]
490: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight shape: [640, 640]
491: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight shape: [640, 640]
492: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight shape: [640, 640]
493: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight shape: [640, 1024]
494: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias shape: [640]
495: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight shape: [640, 640]
496: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight shape: [640, 640]
497: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight shape: [640, 1024]
498: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias shape: [5120]
499: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight shape: [5120, 640]
500: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias shape: [640]
501: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight shape: [640, 2560]
502: up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias shape: [640]
503: up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight shape: [640]
504: up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias shape: [640]
505: up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight shape: [640]
506: up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias shape: [640]
507: up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight shape: [640]
508: up_blocks.2.attentions.2.norm.bias shape: [640]
509: up_blocks.2.attentions.2.norm.weight shape: [640]
510: up_blocks.2.attentions.2.proj_in.bias shape: [640]
511: up_blocks.2.attentions.2.proj_in.weight shape: [640, 640]
512: up_blocks.2.attentions.2.proj_out.bias shape: [640]
513: up_blocks.2.attentions.2.proj_out.weight shape: [640, 640]
514: up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight shape: [640, 640]
515: up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias shape: [640]
516: up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight shape: [640, 640]
517: up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight shape: [640, 640]
518: up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight shape: [640, 640]
519: up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight shape: [640, 1024]
520: up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias shape: [640]
521: up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight shape: [640, 640]
522: up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight shape: [640, 640]
523: up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight shape: [640, 1024]
524: up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias shape: [5120]
525: up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight shape: [5120, 640]
526: up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias shape: [640]
527: up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight shape: [640, 2560]
528: up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias shape: [640]
529: up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight shape: [640]
530: up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias shape: [640]
531: up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight shape: [640]
532: up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias shape: [640]
533: up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight shape: [640]
534: up_blocks.2.resnets.0.conv1.bias shape: [640]
535: up_blocks.2.resnets.0.conv1.weight shape: [640, 1920, 3, 3]
536: up_blocks.2.resnets.0.conv2.bias shape: [640]
537: up_blocks.2.resnets.0.conv2.weight shape: [640, 640, 3, 3]
538: up_blocks.2.resnets.0.conv_shortcut.bias shape: [640]
539: up_blocks.2.resnets.0.conv_shortcut.weight shape: [640, 1920, 1, 1]
540: up_blocks.2.resnets.0.norm1.bias shape: [1920]
541: up_blocks.2.resnets.0.norm1.weight shape: [1920]
542: up_blocks.2.resnets.0.norm2.bias shape: [640]
543: up_blocks.2.resnets.0.norm2.weight shape: [640]
544: up_blocks.2.resnets.0.time_emb_proj.bias shape: [640]
545: up_blocks.2.resnets.0.time_emb_proj.weight shape: [640, 1280]
546: up_blocks.2.resnets.1.conv1.bias shape: [640]
547: up_blocks.2.resnets.1.conv1.weight shape: [640, 1280, 3, 3]
548: up_blocks.2.resnets.1.conv2.bias shape: [640]
549: up_blocks.2.resnets.1.conv2.weight shape: [640, 640, 3, 3]
550: up_blocks.2.resnets.1.conv_shortcut.bias shape: [640]
551: up_blocks.2.resnets.1.conv_shortcut.weight shape: [640, 1280, 1, 1]
552: up_blocks.2.resnets.1.norm1.bias shape: [1280]
553: up_blocks.2.resnets.1.norm1.weight shape: [1280]
554: up_blocks.2.resnets.1.norm2.bias shape: [640]
555: up_blocks.2.resnets.1.norm2.weight shape: [640]
556: up_blocks.2.resnets.1.time_emb_proj.bias shape: [640]
557: up_blocks.2.resnets.1.time_emb_proj.weight shape: [640, 1280]
558: up_blocks.2.resnets.2.conv1.bias shape: [640]
559: up_blocks.2.resnets.2.conv1.weight shape: [640, 960, 3, 3]
560: up_blocks.2.resnets.2.conv2.bias shape: [640]
561: up_blocks.2.resnets.2.conv2.weight shape: [640, 640, 3, 3]
562: up_blocks.2.resnets.2.conv_shortcut.bias shape: [640]
563: up_blocks.2.resnets.2.conv_shortcut.weight shape: [640, 960, 1, 1]
564: up_blocks.2.resnets.2.norm1.bias shape: [960]
565: up_blocks.2.resnets.2.norm1.weight shape: [960]
566: up_blocks.2.resnets.2.norm2.bias shape: [640]
567: up_blocks.2.resnets.2.norm2.weight shape: [640]
568: up_blocks.2.resnets.2.time_emb_proj.bias shape: [640]
569: up_blocks.2.resnets.2.time_emb_proj.weight shape: [640, 1280]
570: up_blocks.2.upsamplers.0.conv.bias shape: [640]
571: up_blocks.2.upsamplers.0.conv.weight shape: [640, 640, 3, 3]
572: up_blocks.3.attentions.0.norm.bias shape: [320]
573: up_blocks.3.attentions.0.norm.weight shape: [320]
574: up_blocks.3.attentions.0.proj_in.bias shape: [320]
575: up_blocks.3.attentions.0.proj_in.weight shape: [320, 320]
576: up_blocks.3.attentions.0.proj_out.bias shape: [320]
577: up_blocks.3.attentions.0.proj_out.weight shape: [320, 320]
578: up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight shape: [320, 320]
579: up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias shape: [320]
580: up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight shape: [320, 320]
581: up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight shape: [320, 320]
582: up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight shape: [320, 320]
583: up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight shape: [320, 1024]
584: up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias shape: [320]
585: up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight shape: [320, 320]
586: up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight shape: [320, 320]
587: up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight shape: [320, 1024]
588: up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias shape: [2560]
589: up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight shape: [2560, 320]
590: up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias shape: [320]
591: up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight shape: [320, 1280]
592: up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias shape: [320]
593: up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight shape: [320]
594: up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias shape: [320]
595: up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight shape: [320]
596: up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias shape: [320]
597: up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight shape: [320]
598: up_blocks.3.attentions.1.norm.bias shape: [320]
599: up_blocks.3.attentions.1.norm.weight shape: [320]
600: up_blocks.3.attentions.1.proj_in.bias shape: [320]
601: up_blocks.3.attentions.1.proj_in.weight shape: [320, 320]
602: up_blocks.3.attentions.1.proj_out.bias shape: [320]
603: up_blocks.3.attentions.1.proj_out.weight shape: [320, 320]
604: up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight shape: [320, 320]
605: up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias shape: [320]
606: up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight shape: [320, 320]
607: up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight shape: [320, 320]
608: up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight shape: [320, 320]
609: up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight shape: [320, 1024]
610: up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias shape: [320]
611: up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight shape: [320, 320]
612: up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight shape: [320, 320]
613: up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight shape: [320, 1024]
614: up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias shape: [2560]
615: up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight shape: [2560, 320]
616: up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias shape: [320]
617: up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight shape: [320, 1280]
618: up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias shape: [320]
619: up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight shape: [320]
620: up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias shape: [320]
621: up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight shape: [320]
622: up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias shape: [320]
623: up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight shape: [320]
624: up_blocks.3.attentions.2.norm.bias shape: [320]
625: up_blocks.3.attentions.2.norm.weight shape: [320]
626: up_blocks.3.attentions.2.proj_in.bias shape: [320]
627: up_blocks.3.attentions.2.proj_in.weight shape: [320, 320]
628: up_blocks.3.attentions.2.proj_out.bias shape: [320]
629: up_blocks.3.attentions.2.proj_out.weight shape: [320, 320]
630: up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight shape: [320, 320]
631: up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias shape: [320]
632: up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight shape: [320, 320]
633: up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight shape: [320, 320]
634: up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight shape: [320, 320]
635: up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight shape: [320, 1024]
636: up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias shape: [320]
637: up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight shape: [320, 320]
638: up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight shape: [320, 320]
639: up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight shape: [320, 1024]
640: up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias shape: [2560]
641: up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight shape: [2560, 320]
642: up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias shape: [320]
643: up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight shape: [320, 1280]
644: up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias shape: [320]
645: up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight shape: [320]
646: up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias shape: [320]
647: up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight shape: [320]
648: up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias shape: [320]
649: up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight shape: [320]
650: up_blocks.3.resnets.0.conv1.bias shape: [320]
651: up_blocks.3.resnets.0.conv1.weight shape: [320, 960, 3, 3]
652: up_blocks.3.resnets.0.conv2.bias shape: [320]
653: up_blocks.3.resnets.0.conv2.weight shape: [320, 320, 3, 3]
654: up_blocks.3.resnets.0.conv_shortcut.bias shape: [320]
655: up_blocks.3.resnets.0.conv_shortcut.weight shape: [320, 960, 1, 1]
656: up_blocks.3.resnets.0.norm1.bias shape: [960]
657: up_blocks.3.resnets.0.norm1.weight shape: [960]
658: up_blocks.3.resnets.0.norm2.bias shape: [320]
659: up_blocks.3.resnets.0.norm2.weight shape: [320]
660: up_blocks.3.resnets.0.time_emb_proj.bias shape: [320]
661: up_blocks.3.resnets.0.time_emb_proj.weight shape: [320, 1280]
662: up_blocks.3.resnets.1.conv1.bias shape: [320]
663: up_blocks.3.resnets.1.conv1.weight shape: [320, 640, 3, 3]
664: up_blocks.3.resnets.1.conv2.bias shape: [320]
665: up_blocks.3.resnets.1.conv2.weight shape: [320, 320, 3, 3]
666: up_blocks.3.resnets.1.conv_shortcut.bias shape: [320]
667: up_blocks.3.resnets.1.conv_shortcut.weight shape: [320, 640, 1, 1]
668: up_blocks.3.resnets.1.norm1.bias shape: [640]
669: up_blocks.3.resnets.1.norm1.weight shape: [640]
670: up_blocks.3.resnets.1.norm2.bias shape: [320]
671: up_blocks.3.resnets.1.norm2.weight shape: [320]
672: up_blocks.3.resnets.1.time_emb_proj.bias shape: [320]
673: up_blocks.3.resnets.1.time_emb_proj.weight shape: [320, 1280]
674: up_blocks.3.resnets.2.conv1.bias shape: [320]
675: up_blocks.3.resnets.2.conv1.weight shape: [320, 640, 3, 3]
676: up_blocks.3.resnets.2.conv2.bias shape: [320]
677: up_blocks.3.resnets.2.conv2.weight shape: [320, 320, 3, 3]
678: up_blocks.3.resnets.2.conv_shortcut.bias shape: [320]
679: up_blocks.3.resnets.2.conv_shortcut.weight shape: [320, 640, 1, 1]
680: up_blocks.3.resnets.2.norm1.bias shape: [640]
681: up_blocks.3.resnets.2.norm1.weight shape: [640]
682: up_blocks.3.resnets.2.norm2.bias shape: [320]
683: up_blocks.3.resnets.2.norm2.weight shape: [320]
684: up_blocks.3.resnets.2.time_emb_proj.bias shape: [320]
685: up_blocks.3.resnets.2.time_emb_proj.weight shape: [320, 1280]
