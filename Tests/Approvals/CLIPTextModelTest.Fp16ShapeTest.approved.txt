0: text_model.embeddings.position_embedding.weight: sum: 203.0545  dtype: Float16 shape: [77,1024]
1: text_model.embeddings.token_embedding.weight: sum: -1188.7434  dtype: Float16 shape: [49408,1024]
2: text_model.encoder.layers.0.layer_norm1.bias: sum: 17.8924  dtype: Float16 shape: [1024]
3: text_model.encoder.layers.0.layer_norm1.weight: sum: 21.3805  dtype: Float16 shape: [1024]
4: text_model.encoder.layers.0.layer_norm2.bias: sum: 23.9205  dtype: Float16 shape: [1024]
5: text_model.encoder.layers.0.layer_norm2.weight: sum: 21.311  dtype: Float16 shape: [1024]
6: text_model.encoder.layers.0.mlp.fc1.bias: sum: 42.7535  dtype: Float16 shape: [4096]
7: text_model.encoder.layers.0.mlp.fc1.weight: sum: 1383.75  dtype: Float16 shape: [4096,1024]
8: text_model.encoder.layers.0.mlp.fc2.bias: sum: 21.8853  dtype: Float16 shape: [1024]
9: text_model.encoder.layers.0.mlp.fc2.weight: sum: 3665.1743  dtype: Float16 shape: [1024,4096]
10: text_model.encoder.layers.0.self_attn.k_proj.bias: sum: 34.0305  dtype: Float16 shape: [1024]
11: text_model.encoder.layers.0.self_attn.k_proj.weight: sum: 810.4756  dtype: Float16 shape: [1024,1024]
12: text_model.encoder.layers.0.self_attn.out_proj.bias: sum: 14.8264  dtype: Float16 shape: [1024]
13: text_model.encoder.layers.0.self_attn.out_proj.weight: sum: 1112.9274  dtype: Float16 shape: [1024,1024]
14: text_model.encoder.layers.0.self_attn.q_proj.bias: sum: 22.4383  dtype: Float16 shape: [1024]
15: text_model.encoder.layers.0.self_attn.q_proj.weight: sum: 650.6512  dtype: Float16 shape: [1024,1024]
16: text_model.encoder.layers.0.self_attn.v_proj.bias: sum: 603.5808  dtype: Float16 shape: [1024]
17: text_model.encoder.layers.0.self_attn.v_proj.weight: sum: 818.713  dtype: Float16 shape: [1024,1024]
18: text_model.encoder.layers.1.layer_norm1.bias: sum: 19.2185  dtype: Float16 shape: [1024]
19: text_model.encoder.layers.1.layer_norm1.weight: sum: 21.2778  dtype: Float16 shape: [1024]
20: text_model.encoder.layers.1.layer_norm2.bias: sum: 19.528  dtype: Float16 shape: [1024]
21: text_model.encoder.layers.1.layer_norm2.weight: sum: 21.3783  dtype: Float16 shape: [1024]
22: text_model.encoder.layers.1.mlp.fc1.bias: sum: 42.719  dtype: Float16 shape: [4096]
23: text_model.encoder.layers.1.mlp.fc1.weight: sum: 1373.9191  dtype: Float16 shape: [4096,1024]
24: text_model.encoder.layers.1.mlp.fc2.bias: sum: 26.8018  dtype: Float16 shape: [1024]
25: text_model.encoder.layers.1.mlp.fc2.weight: sum: 5115.255  dtype: Float16 shape: [1024,4096]
26: text_model.encoder.layers.1.self_attn.k_proj.bias: sum: 27.9257  dtype: Float16 shape: [1024]
27: text_model.encoder.layers.1.self_attn.k_proj.weight: sum: 452.8414  dtype: Float16 shape: [1024,1024]
28: text_model.encoder.layers.1.self_attn.out_proj.bias: sum: 28.5113  dtype: Float16 shape: [1024]
29: text_model.encoder.layers.1.self_attn.out_proj.weight: sum: 1525.0273  dtype: Float16 shape: [1024,1024]
30: text_model.encoder.layers.1.self_attn.q_proj.bias: sum: 26.2769  dtype: Float16 shape: [1024]
31: text_model.encoder.layers.1.self_attn.q_proj.weight: sum: 722.0508  dtype: Float16 shape: [1024,1024]
32: text_model.encoder.layers.1.self_attn.v_proj.bias: sum: 58.0284  dtype: Float16 shape: [1024]
33: text_model.encoder.layers.1.self_attn.v_proj.weight: sum: 1004.3694  dtype: Float16 shape: [1024,1024]
34: text_model.encoder.layers.10.layer_norm1.bias: sum: 19.794  dtype: Float16 shape: [1024]
35: text_model.encoder.layers.10.layer_norm1.weight: sum: 21.2908  dtype: Float16 shape: [1024]
36: text_model.encoder.layers.10.layer_norm2.bias: sum: -19.2749  dtype: Float16 shape: [1024]
37: text_model.encoder.layers.10.layer_norm2.weight: sum: 21.3168  dtype: Float16 shape: [1024]
38: text_model.encoder.layers.10.mlp.fc1.bias: sum: 42.6526  dtype: Float16 shape: [4096]
39: text_model.encoder.layers.10.mlp.fc1.weight: sum: 1484.9277  dtype: Float16 shape: [4096,1024]
40: text_model.encoder.layers.10.mlp.fc2.bias: sum: 3.0606  dtype: Float16 shape: [1024]
41: text_model.encoder.layers.10.mlp.fc2.weight: sum: 435.9638  dtype: Float16 shape: [1024,4096]
42: text_model.encoder.layers.10.self_attn.k_proj.bias: sum: 10.1419  dtype: Float16 shape: [1024]
43: text_model.encoder.layers.10.self_attn.k_proj.weight: sum: -335.7949  dtype: Float16 shape: [1024,1024]
44: text_model.encoder.layers.10.self_attn.out_proj.bias: sum: 30.2504  dtype: Float16 shape: [1024]
45: text_model.encoder.layers.10.self_attn.out_proj.weight: sum: 2241.5566  dtype: Float16 shape: [1024,1024]
46: text_model.encoder.layers.10.self_attn.q_proj.bias: sum: 167.9184  dtype: Float16 shape: [1024]
47: text_model.encoder.layers.10.self_attn.q_proj.weight: sum: 263.6676  dtype: Float16 shape: [1024,1024]
48: text_model.encoder.layers.10.self_attn.v_proj.bias: sum: 34.5594  dtype: Float16 shape: [1024]
49: text_model.encoder.layers.10.self_attn.v_proj.weight: sum: 557.3188  dtype: Float16 shape: [1024,1024]
50: text_model.encoder.layers.11.layer_norm1.bias: sum: 19.8674  dtype: Float16 shape: [1024]
51: text_model.encoder.layers.11.layer_norm1.weight: sum: 21.3212  dtype: Float16 shape: [1024]
52: text_model.encoder.layers.11.layer_norm2.bias: sum: 18.6231  dtype: Float16 shape: [1024]
53: text_model.encoder.layers.11.layer_norm2.weight: sum: 21.3325  dtype: Float16 shape: [1024]
54: text_model.encoder.layers.11.mlp.fc1.bias: sum: 42.7258  dtype: Float16 shape: [4096]
55: text_model.encoder.layers.11.mlp.fc1.weight: sum: 1365.1249  dtype: Float16 shape: [4096,1024]
56: text_model.encoder.layers.11.mlp.fc2.bias: sum: 11.8027  dtype: Float16 shape: [1024]
57: text_model.encoder.layers.11.mlp.fc2.weight: sum: 674.8176  dtype: Float16 shape: [1024,4096]
58: text_model.encoder.layers.11.self_attn.k_proj.bias: sum: 49.4108  dtype: Float16 shape: [1024]
59: text_model.encoder.layers.11.self_attn.k_proj.weight: sum: 547.4852  dtype: Float16 shape: [1024,1024]
60: text_model.encoder.layers.11.self_attn.out_proj.bias: sum: 45.9878  dtype: Float16 shape: [1024]
61: text_model.encoder.layers.11.self_attn.out_proj.weight: sum: 131.446  dtype: Float16 shape: [1024,1024]
62: text_model.encoder.layers.11.self_attn.q_proj.bias: sum: 16.7341  dtype: Float16 shape: [1024]
63: text_model.encoder.layers.11.self_attn.q_proj.weight: sum: -473.3487  dtype: Float16 shape: [1024,1024]
64: text_model.encoder.layers.11.self_attn.v_proj.bias: sum: 40.7444  dtype: Float16 shape: [1024]
65: text_model.encoder.layers.11.self_attn.v_proj.weight: sum: 809.8776  dtype: Float16 shape: [1024,1024]
66: text_model.encoder.layers.12.layer_norm1.bias: sum: 19.6804  dtype: Float16 shape: [1024]
67: text_model.encoder.layers.12.layer_norm1.weight: sum: 21.3304  dtype: Float16 shape: [1024]
68: text_model.encoder.layers.12.layer_norm2.bias: sum: 27.6389  dtype: Float16 shape: [1024]
69: text_model.encoder.layers.12.layer_norm2.weight: sum: 21.3377  dtype: Float16 shape: [1024]
70: text_model.encoder.layers.12.mlp.fc1.bias: sum: 42.7355  dtype: Float16 shape: [4096]
71: text_model.encoder.layers.12.mlp.fc1.weight: sum: 1230.724  dtype: Float16 shape: [4096,1024]
72: text_model.encoder.layers.12.mlp.fc2.bias: sum: 29.3736  dtype: Float16 shape: [1024]
73: text_model.encoder.layers.12.mlp.fc2.weight: sum: 2306.543  dtype: Float16 shape: [1024,4096]
74: text_model.encoder.layers.12.self_attn.k_proj.bias: sum: 74.1688  dtype: Float16 shape: [1024]
75: text_model.encoder.layers.12.self_attn.k_proj.weight: sum: 712.6513  dtype: Float16 shape: [1024,1024]
76: text_model.encoder.layers.12.self_attn.out_proj.bias: sum: 17.573  dtype: Float16 shape: [1024]
77: text_model.encoder.layers.12.self_attn.out_proj.weight: sum: 201.7078  dtype: Float16 shape: [1024,1024]
78: text_model.encoder.layers.12.self_attn.q_proj.bias: sum: 2.0517  dtype: Float16 shape: [1024]
79: text_model.encoder.layers.12.self_attn.q_proj.weight: sum: 613.369  dtype: Float16 shape: [1024,1024]
80: text_model.encoder.layers.12.self_attn.v_proj.bias: sum: -11.536  dtype: Float16 shape: [1024]
81: text_model.encoder.layers.12.self_attn.v_proj.weight: sum: 384.0717  dtype: Float16 shape: [1024,1024]
82: text_model.encoder.layers.13.layer_norm1.bias: sum: 19.6694  dtype: Float16 shape: [1024]
83: text_model.encoder.layers.13.layer_norm1.weight: sum: 21.3209  dtype: Float16 shape: [1024]
84: text_model.encoder.layers.13.layer_norm2.bias: sum: 24.5291  dtype: Float16 shape: [1024]
85: text_model.encoder.layers.13.layer_norm2.weight: sum: 21.3352  dtype: Float16 shape: [1024]
86: text_model.encoder.layers.13.mlp.fc1.bias: sum: 42.5886  dtype: Float16 shape: [4096]
87: text_model.encoder.layers.13.mlp.fc1.weight: sum: 1365.8198  dtype: Float16 shape: [4096,1024]
88: text_model.encoder.layers.13.mlp.fc2.bias: sum: 21.4544  dtype: Float16 shape: [1024]
89: text_model.encoder.layers.13.mlp.fc2.weight: sum: -727.094  dtype: Float16 shape: [1024,4096]
90: text_model.encoder.layers.13.self_attn.k_proj.bias: sum: 20.8313  dtype: Float16 shape: [1024]
91: text_model.encoder.layers.13.self_attn.k_proj.weight: sum: 618.4786  dtype: Float16 shape: [1024,1024]
92: text_model.encoder.layers.13.self_attn.out_proj.bias: sum: 18.8132  dtype: Float16 shape: [1024]
93: text_model.encoder.layers.13.self_attn.out_proj.weight: sum: 3716.4492  dtype: Float16 shape: [1024,1024]
94: text_model.encoder.layers.13.self_attn.q_proj.bias: sum: 19.9311  dtype: Float16 shape: [1024]
95: text_model.encoder.layers.13.self_attn.q_proj.weight: sum: 657.0475  dtype: Float16 shape: [1024,1024]
96: text_model.encoder.layers.13.self_attn.v_proj.bias: sum: 12.6494  dtype: Float16 shape: [1024]
97: text_model.encoder.layers.13.self_attn.v_proj.weight: sum: 797.6338  dtype: Float16 shape: [1024,1024]
98: text_model.encoder.layers.14.layer_norm1.bias: sum: 19.5572  dtype: Float16 shape: [1024]
99: text_model.encoder.layers.14.layer_norm1.weight: sum: 21.3262  dtype: Float16 shape: [1024]
100: text_model.encoder.layers.14.layer_norm2.bias: sum: 24.9529  dtype: Float16 shape: [1024]
101: text_model.encoder.layers.14.layer_norm2.weight: sum: 21.3474  dtype: Float16 shape: [1024]
102: text_model.encoder.layers.14.mlp.fc1.bias: sum: 42.5564  dtype: Float16 shape: [4096]
103: text_model.encoder.layers.14.mlp.fc1.weight: sum: 1363.8447  dtype: Float16 shape: [4096,1024]
104: text_model.encoder.layers.14.mlp.fc2.bias: sum: 22.9168  dtype: Float16 shape: [1024]
105: text_model.encoder.layers.14.mlp.fc2.weight: sum: 256.2779  dtype: Float16 shape: [1024,4096]
106: text_model.encoder.layers.14.self_attn.k_proj.bias: sum: 17.887  dtype: Float16 shape: [1024]
107: text_model.encoder.layers.14.self_attn.k_proj.weight: sum: 321.2519  dtype: Float16 shape: [1024,1024]
108: text_model.encoder.layers.14.self_attn.out_proj.bias: sum: 102.0082  dtype: Float16 shape: [1024]
109: text_model.encoder.layers.14.self_attn.out_proj.weight: sum: 20814.814  dtype: Float16 shape: [1024,1024]
110: text_model.encoder.layers.14.self_attn.q_proj.bias: sum: 11.451  dtype: Float16 shape: [1024]
111: text_model.encoder.layers.14.self_attn.q_proj.weight: sum: 651.654  dtype: Float16 shape: [1024,1024]
112: text_model.encoder.layers.14.self_attn.v_proj.bias: sum: 26.9006  dtype: Float16 shape: [1024]
113: text_model.encoder.layers.14.self_attn.v_proj.weight: sum: 774.4118  dtype: Float16 shape: [1024,1024]
114: text_model.encoder.layers.15.layer_norm1.bias: sum: 19.3209  dtype: Float16 shape: [1024]
115: text_model.encoder.layers.15.layer_norm1.weight: sum: 21.34  dtype: Float16 shape: [1024]
116: text_model.encoder.layers.15.layer_norm2.bias: sum: 30.1082  dtype: Float16 shape: [1024]
117: text_model.encoder.layers.15.layer_norm2.weight: sum: 21.328  dtype: Float16 shape: [1024]
118: text_model.encoder.layers.15.mlp.fc1.bias: sum: 42.6542  dtype: Float16 shape: [4096]
119: text_model.encoder.layers.15.mlp.fc1.weight: sum: 1363.3329  dtype: Float16 shape: [4096,1024]
120: text_model.encoder.layers.15.mlp.fc2.bias: sum: 50.3008  dtype: Float16 shape: [1024]
121: text_model.encoder.layers.15.mlp.fc2.weight: sum: 1498.5375  dtype: Float16 shape: [1024,4096]
122: text_model.encoder.layers.15.self_attn.k_proj.bias: sum: 33.2115  dtype: Float16 shape: [1024]
123: text_model.encoder.layers.15.self_attn.k_proj.weight: sum: 938.9045  dtype: Float16 shape: [1024,1024]
124: text_model.encoder.layers.15.self_attn.out_proj.bias: sum: 19.8365  dtype: Float16 shape: [1024]
125: text_model.encoder.layers.15.self_attn.out_proj.weight: sum: 2158.4104  dtype: Float16 shape: [1024,1024]
126: text_model.encoder.layers.15.self_attn.q_proj.bias: sum: 30.6337  dtype: Float16 shape: [1024]
127: text_model.encoder.layers.15.self_attn.q_proj.weight: sum: 866.5556  dtype: Float16 shape: [1024,1024]
128: text_model.encoder.layers.15.self_attn.v_proj.bias: sum: 15.0729  dtype: Float16 shape: [1024]
129: text_model.encoder.layers.15.self_attn.v_proj.weight: sum: 960.183  dtype: Float16 shape: [1024,1024]
130: text_model.encoder.layers.16.layer_norm1.bias: sum: 19.6985  dtype: Float16 shape: [1024]
131: text_model.encoder.layers.16.layer_norm1.weight: sum: 21.32  dtype: Float16 shape: [1024]
132: text_model.encoder.layers.16.layer_norm2.bias: sum: 23.7913  dtype: Float16 shape: [1024]
133: text_model.encoder.layers.16.layer_norm2.weight: sum: 21.3454  dtype: Float16 shape: [1024]
134: text_model.encoder.layers.16.mlp.fc1.bias: sum: 42.582  dtype: Float16 shape: [4096]
135: text_model.encoder.layers.16.mlp.fc1.weight: sum: 1369.2875  dtype: Float16 shape: [4096,1024]
136: text_model.encoder.layers.16.mlp.fc2.bias: sum: 55.8382  dtype: Float16 shape: [1024]
137: text_model.encoder.layers.16.mlp.fc2.weight: sum: 1078.6873  dtype: Float16 shape: [1024,4096]
138: text_model.encoder.layers.16.self_attn.k_proj.bias: sum: -131.1926  dtype: Float16 shape: [1024]
139: text_model.encoder.layers.16.self_attn.k_proj.weight: sum: 879.2949  dtype: Float16 shape: [1024,1024]
140: text_model.encoder.layers.16.self_attn.out_proj.bias: sum: 41.3845  dtype: Float16 shape: [1024]
141: text_model.encoder.layers.16.self_attn.out_proj.weight: sum: 1086.7949  dtype: Float16 shape: [1024,1024]
142: text_model.encoder.layers.16.self_attn.q_proj.bias: sum: 39.9185  dtype: Float16 shape: [1024]
143: text_model.encoder.layers.16.self_attn.q_proj.weight: sum: -151.3288  dtype: Float16 shape: [1024,1024]
144: text_model.encoder.layers.16.self_attn.v_proj.bias: sum: 25.632  dtype: Float16 shape: [1024]
145: text_model.encoder.layers.16.self_attn.v_proj.weight: sum: 1084.7742  dtype: Float16 shape: [1024,1024]
146: text_model.encoder.layers.17.layer_norm1.bias: sum: 19.24  dtype: Float16 shape: [1024]
147: text_model.encoder.layers.17.layer_norm1.weight: sum: 21.3211  dtype: Float16 shape: [1024]
148: text_model.encoder.layers.17.layer_norm2.bias: sum: 23.3518  dtype: Float16 shape: [1024]
149: text_model.encoder.layers.17.layer_norm2.weight: sum: 21.3318  dtype: Float16 shape: [1024]
150: text_model.encoder.layers.17.mlp.fc1.bias: sum: 42.7028  dtype: Float16 shape: [4096]
151: text_model.encoder.layers.17.mlp.fc1.weight: sum: 1368.6392  dtype: Float16 shape: [4096,1024]
152: text_model.encoder.layers.17.mlp.fc2.bias: sum: 3.658  dtype: Float16 shape: [1024]
153: text_model.encoder.layers.17.mlp.fc2.weight: sum: 1505.0875  dtype: Float16 shape: [1024,4096]
154: text_model.encoder.layers.17.self_attn.k_proj.bias: sum: 9.7151  dtype: Float16 shape: [1024]
155: text_model.encoder.layers.17.self_attn.k_proj.weight: sum: 327.2672  dtype: Float16 shape: [1024,1024]
156: text_model.encoder.layers.17.self_attn.out_proj.bias: sum: 16.6785  dtype: Float16 shape: [1024]
157: text_model.encoder.layers.17.self_attn.out_proj.weight: sum: 16423.637  dtype: Float16 shape: [1024,1024]
158: text_model.encoder.layers.17.self_attn.q_proj.bias: sum: 30.9983  dtype: Float16 shape: [1024]
159: text_model.encoder.layers.17.self_attn.q_proj.weight: sum: 1091.9553  dtype: Float16 shape: [1024,1024]
160: text_model.encoder.layers.17.self_attn.v_proj.bias: sum: 16.0511  dtype: Float16 shape: [1024]
161: text_model.encoder.layers.17.self_attn.v_proj.weight: sum: 705.6474  dtype: Float16 shape: [1024,1024]
162: text_model.encoder.layers.18.layer_norm1.bias: sum: 19.0406  dtype: Float16 shape: [1024]
163: text_model.encoder.layers.18.layer_norm1.weight: sum: 21.3308  dtype: Float16 shape: [1024]
164: text_model.encoder.layers.18.layer_norm2.bias: sum: 23.1338  dtype: Float16 shape: [1024]
165: text_model.encoder.layers.18.layer_norm2.weight: sum: 21.332  dtype: Float16 shape: [1024]
166: text_model.encoder.layers.18.mlp.fc1.bias: sum: 42.6451  dtype: Float16 shape: [4096]
167: text_model.encoder.layers.18.mlp.fc1.weight: sum: 1366.4847  dtype: Float16 shape: [4096,1024]
168: text_model.encoder.layers.18.mlp.fc2.bias: sum: -24.8543  dtype: Float16 shape: [1024]
169: text_model.encoder.layers.18.mlp.fc2.weight: sum: 2266.086  dtype: Float16 shape: [1024,4096]
170: text_model.encoder.layers.18.self_attn.k_proj.bias: sum: 23.2757  dtype: Float16 shape: [1024]
171: text_model.encoder.layers.18.self_attn.k_proj.weight: sum: 796.3325  dtype: Float16 shape: [1024,1024]
172: text_model.encoder.layers.18.self_attn.out_proj.bias: sum: 30.9917  dtype: Float16 shape: [1024]
173: text_model.encoder.layers.18.self_attn.out_proj.weight: sum: -5767.9224  dtype: Float16 shape: [1024,1024]
174: text_model.encoder.layers.18.self_attn.q_proj.bias: sum: 24.2645  dtype: Float16 shape: [1024]
175: text_model.encoder.layers.18.self_attn.q_proj.weight: sum: -3164.9587  dtype: Float16 shape: [1024,1024]
176: text_model.encoder.layers.18.self_attn.v_proj.bias: sum: 31.6547  dtype: Float16 shape: [1024]
177: text_model.encoder.layers.18.self_attn.v_proj.weight: sum: 851.022  dtype: Float16 shape: [1024,1024]
178: text_model.encoder.layers.19.layer_norm1.bias: sum: 19.1262  dtype: Float16 shape: [1024]
179: text_model.encoder.layers.19.layer_norm1.weight: sum: 21.3238  dtype: Float16 shape: [1024]
180: text_model.encoder.layers.19.layer_norm2.bias: sum: 20.9901  dtype: Float16 shape: [1024]
181: text_model.encoder.layers.19.layer_norm2.weight: sum: 21.3194  dtype: Float16 shape: [1024]
182: text_model.encoder.layers.19.mlp.fc1.bias: sum: 42.6946  dtype: Float16 shape: [4096]
183: text_model.encoder.layers.19.mlp.fc1.weight: sum: 1362.9242  dtype: Float16 shape: [4096,1024]
184: text_model.encoder.layers.19.mlp.fc2.bias: sum: -209.0104  dtype: Float16 shape: [1024]
185: text_model.encoder.layers.19.mlp.fc2.weight: sum: 3235.5364  dtype: Float16 shape: [1024,4096]
186: text_model.encoder.layers.19.self_attn.k_proj.bias: sum: 14.1244  dtype: Float16 shape: [1024]
187: text_model.encoder.layers.19.self_attn.k_proj.weight: sum: 400.0474  dtype: Float16 shape: [1024,1024]
188: text_model.encoder.layers.19.self_attn.out_proj.bias: sum: 42.8957  dtype: Float16 shape: [1024]
189: text_model.encoder.layers.19.self_attn.out_proj.weight: sum: 378.0308  dtype: Float16 shape: [1024,1024]
190: text_model.encoder.layers.19.self_attn.q_proj.bias: sum: 16.8617  dtype: Float16 shape: [1024]
191: text_model.encoder.layers.19.self_attn.q_proj.weight: sum: 339.6962  dtype: Float16 shape: [1024,1024]
192: text_model.encoder.layers.19.self_attn.v_proj.bias: sum: 37.9336  dtype: Float16 shape: [1024]
193: text_model.encoder.layers.19.self_attn.v_proj.weight: sum: -4629.1846  dtype: Float16 shape: [1024,1024]
194: text_model.encoder.layers.2.layer_norm1.bias: sum: 19.12  dtype: Float16 shape: [1024]
195: text_model.encoder.layers.2.layer_norm1.weight: sum: 21.253  dtype: Float16 shape: [1024]
196: text_model.encoder.layers.2.layer_norm2.bias: sum: 24.7493  dtype: Float16 shape: [1024]
197: text_model.encoder.layers.2.layer_norm2.weight: sum: 21.3318  dtype: Float16 shape: [1024]
198: text_model.encoder.layers.2.mlp.fc1.bias: sum: 42.7902  dtype: Float16 shape: [4096]
199: text_model.encoder.layers.2.mlp.fc1.weight: sum: 1362.0402  dtype: Float16 shape: [4096,1024]
200: text_model.encoder.layers.2.mlp.fc2.bias: sum: 25.5409  dtype: Float16 shape: [1024]
201: text_model.encoder.layers.2.mlp.fc2.weight: sum: 11779.333  dtype: Float16 shape: [1024,4096]
202: text_model.encoder.layers.2.self_attn.k_proj.bias: sum: 20.4344  dtype: Float16 shape: [1024]
203: text_model.encoder.layers.2.self_attn.k_proj.weight: sum: 1011.2742  dtype: Float16 shape: [1024,1024]
204: text_model.encoder.layers.2.self_attn.out_proj.bias: sum: 25.0229  dtype: Float16 shape: [1024]
205: text_model.encoder.layers.2.self_attn.out_proj.weight: sum: -3334.637  dtype: Float16 shape: [1024,1024]
206: text_model.encoder.layers.2.self_attn.q_proj.bias: sum: 27.7451  dtype: Float16 shape: [1024]
207: text_model.encoder.layers.2.self_attn.q_proj.weight: sum: -6893.6514  dtype: Float16 shape: [1024,1024]
208: text_model.encoder.layers.2.self_attn.v_proj.bias: sum: 26.0526  dtype: Float16 shape: [1024]
209: text_model.encoder.layers.2.self_attn.v_proj.weight: sum: 1450.7225  dtype: Float16 shape: [1024,1024]
210: text_model.encoder.layers.20.layer_norm1.bias: sum: 19.3044  dtype: Float16 shape: [1024]
211: text_model.encoder.layers.20.layer_norm1.weight: sum: 21.3165  dtype: Float16 shape: [1024]
212: text_model.encoder.layers.20.layer_norm2.bias: sum: 20.5385  dtype: Float16 shape: [1024]
213: text_model.encoder.layers.20.layer_norm2.weight: sum: 21.3312  dtype: Float16 shape: [1024]
214: text_model.encoder.layers.20.mlp.fc1.bias: sum: 42.6824  dtype: Float16 shape: [4096]
215: text_model.encoder.layers.20.mlp.fc1.weight: sum: 1363.9927  dtype: Float16 shape: [4096,1024]
216: text_model.encoder.layers.20.mlp.fc2.bias: sum: 96.2016  dtype: Float16 shape: [1024]
217: text_model.encoder.layers.20.mlp.fc2.weight: sum: 2298.977  dtype: Float16 shape: [1024,4096]
218: text_model.encoder.layers.20.self_attn.k_proj.bias: sum: 30.377  dtype: Float16 shape: [1024]
219: text_model.encoder.layers.20.self_attn.k_proj.weight: sum: 474.0965  dtype: Float16 shape: [1024,1024]
220: text_model.encoder.layers.20.self_attn.out_proj.bias: sum: 25.5384  dtype: Float16 shape: [1024]
221: text_model.encoder.layers.20.self_attn.out_proj.weight: sum: -157622.52  dtype: Float16 shape: [1024,1024]
222: text_model.encoder.layers.20.self_attn.q_proj.bias: sum: 19.587  dtype: Float16 shape: [1024]
223: text_model.encoder.layers.20.self_attn.q_proj.weight: sum: 726.5036  dtype: Float16 shape: [1024,1024]
224: text_model.encoder.layers.20.self_attn.v_proj.bias: sum: 20.691  dtype: Float16 shape: [1024]
225: text_model.encoder.layers.20.self_attn.v_proj.weight: sum: 705.0926  dtype: Float16 shape: [1024,1024]
226: text_model.encoder.layers.21.layer_norm1.bias: sum: 19.3534  dtype: Float16 shape: [1024]
227: text_model.encoder.layers.21.layer_norm1.weight: sum: 21.307  dtype: Float16 shape: [1024]
228: text_model.encoder.layers.21.layer_norm2.bias: sum: 25.0809  dtype: Float16 shape: [1024]
229: text_model.encoder.layers.21.layer_norm2.weight: sum: 21.3132  dtype: Float16 shape: [1024]
230: text_model.encoder.layers.21.mlp.fc1.bias: sum: 42.6681  dtype: Float16 shape: [4096]
231: text_model.encoder.layers.21.mlp.fc1.weight: sum: 1370.6609  dtype: Float16 shape: [4096,1024]
232: text_model.encoder.layers.21.mlp.fc2.bias: sum: -122.9189  dtype: Float16 shape: [1024]
233: text_model.encoder.layers.21.mlp.fc2.weight: sum: -266.19  dtype: Float16 shape: [1024,4096]
234: text_model.encoder.layers.21.self_attn.k_proj.bias: sum: 137.6786  dtype: Float16 shape: [1024]
235: text_model.encoder.layers.21.self_attn.k_proj.weight: sum: 766.831  dtype: Float16 shape: [1024,1024]
236: text_model.encoder.layers.21.self_attn.out_proj.bias: sum: 31.5803  dtype: Float16 shape: [1024]
237: text_model.encoder.layers.21.self_attn.out_proj.weight: sum: 4984.8774  dtype: Float16 shape: [1024,1024]
238: text_model.encoder.layers.21.self_attn.q_proj.bias: sum: 43.5721  dtype: Float16 shape: [1024]
239: text_model.encoder.layers.21.self_attn.q_proj.weight: sum: -1190.1187  dtype: Float16 shape: [1024,1024]
240: text_model.encoder.layers.21.self_attn.v_proj.bias: sum: 25.172  dtype: Float16 shape: [1024]
241: text_model.encoder.layers.21.self_attn.v_proj.weight: sum: 462.329  dtype: Float16 shape: [1024,1024]
242: text_model.encoder.layers.22.layer_norm1.bias: sum: 19.1185  dtype: Float16 shape: [1024]
243: text_model.encoder.layers.22.layer_norm1.weight: sum: 21.3101  dtype: Float16 shape: [1024]
244: text_model.encoder.layers.22.layer_norm2.bias: sum: 22.9287  dtype: Float16 shape: [1024]
245: text_model.encoder.layers.22.layer_norm2.weight: sum: 21.3211  dtype: Float16 shape: [1024]
246: text_model.encoder.layers.22.mlp.fc1.bias: sum: 42.6194  dtype: Float16 shape: [4096]
247: text_model.encoder.layers.22.mlp.fc1.weight: sum: 1367.3253  dtype: Float16 shape: [4096,1024]
248: text_model.encoder.layers.22.mlp.fc2.bias: sum: 11.6149  dtype: Float16 shape: [1024]
249: text_model.encoder.layers.22.mlp.fc2.weight: sum: 882.8137  dtype: Float16 shape: [1024,4096]
250: text_model.encoder.layers.22.self_attn.k_proj.bias: sum: 25.5226  dtype: Float16 shape: [1024]
251: text_model.encoder.layers.22.self_attn.k_proj.weight: sum: 1171.1149  dtype: Float16 shape: [1024,1024]
252: text_model.encoder.layers.22.self_attn.out_proj.bias: sum: 250.1311  dtype: Float16 shape: [1024]
253: text_model.encoder.layers.22.self_attn.out_proj.weight: sum: 58.4847  dtype: Float16 shape: [1024,1024]
254: text_model.encoder.layers.22.self_attn.q_proj.bias: sum: 23.8367  dtype: Float16 shape: [1024]
255: text_model.encoder.layers.22.self_attn.q_proj.weight: sum: 925.7639  dtype: Float16 shape: [1024,1024]
256: text_model.encoder.layers.22.self_attn.v_proj.bias: sum: 19.12  dtype: Float16 shape: [1024]
257: text_model.encoder.layers.22.self_attn.v_proj.weight: sum: 517.8686  dtype: Float16 shape: [1024,1024]
258: text_model.encoder.layers.3.layer_norm1.bias: sum: 19.4965  dtype: Float16 shape: [1024]
259: text_model.encoder.layers.3.layer_norm1.weight: sum: 21.2684  dtype: Float16 shape: [1024]
260: text_model.encoder.layers.3.layer_norm2.bias: sum: 36.8873  dtype: Float16 shape: [1024]
261: text_model.encoder.layers.3.layer_norm2.weight: sum: 21.3474  dtype: Float16 shape: [1024]
262: text_model.encoder.layers.3.mlp.fc1.bias: sum: 42.6179  dtype: Float16 shape: [4096]
263: text_model.encoder.layers.3.mlp.fc1.weight: sum: 1358.8031  dtype: Float16 shape: [4096,1024]
264: text_model.encoder.layers.3.mlp.fc2.bias: sum: 13.3887  dtype: Float16 shape: [1024]
265: text_model.encoder.layers.3.mlp.fc2.weight: sum: 5256.5635  dtype: Float16 shape: [1024,4096]
266: text_model.encoder.layers.3.self_attn.k_proj.bias: sum: 14.4986  dtype: Float16 shape: [1024]
267: text_model.encoder.layers.3.self_attn.k_proj.weight: sum: 1449.467  dtype: Float16 shape: [1024,1024]
268: text_model.encoder.layers.3.self_attn.out_proj.bias: sum: 70.0261  dtype: Float16 shape: [1024]
269: text_model.encoder.layers.3.self_attn.out_proj.weight: sum: -800.3669  dtype: Float16 shape: [1024,1024]
270: text_model.encoder.layers.3.self_attn.q_proj.bias: sum: 139.093  dtype: Float16 shape: [1024]
271: text_model.encoder.layers.3.self_attn.q_proj.weight: sum: 690.6498  dtype: Float16 shape: [1024,1024]
272: text_model.encoder.layers.3.self_attn.v_proj.bias: sum: 24.8283  dtype: Float16 shape: [1024]
273: text_model.encoder.layers.3.self_attn.v_proj.weight: sum: 487.7648  dtype: Float16 shape: [1024,1024]
274: text_model.encoder.layers.4.layer_norm1.bias: sum: 19.1382  dtype: Float16 shape: [1024]
275: text_model.encoder.layers.4.layer_norm1.weight: sum: 21.2649  dtype: Float16 shape: [1024]
276: text_model.encoder.layers.4.layer_norm2.bias: sum: 25.0504  dtype: Float16 shape: [1024]
277: text_model.encoder.layers.4.layer_norm2.weight: sum: 21.3549  dtype: Float16 shape: [1024]
278: text_model.encoder.layers.4.mlp.fc1.bias: sum: 42.6577  dtype: Float16 shape: [4096]
279: text_model.encoder.layers.4.mlp.fc1.weight: sum: 1369.0734  dtype: Float16 shape: [4096,1024]
280: text_model.encoder.layers.4.mlp.fc2.bias: sum: 30.0161  dtype: Float16 shape: [1024]
281: text_model.encoder.layers.4.mlp.fc2.weight: sum: 678.0658  dtype: Float16 shape: [1024,4096]
282: text_model.encoder.layers.4.self_attn.k_proj.bias: sum: 17.8454  dtype: Float16 shape: [1024]
283: text_model.encoder.layers.4.self_attn.k_proj.weight: sum: 888.1975  dtype: Float16 shape: [1024,1024]
284: text_model.encoder.layers.4.self_attn.out_proj.bias: sum: 24.795  dtype: Float16 shape: [1024]
285: text_model.encoder.layers.4.self_attn.out_proj.weight: sum: -1681.1174  dtype: Float16 shape: [1024,1024]
286: text_model.encoder.layers.4.self_attn.q_proj.bias: sum: 25.8093  dtype: Float16 shape: [1024]
287: text_model.encoder.layers.4.self_attn.q_proj.weight: sum: 604.792  dtype: Float16 shape: [1024,1024]
288: text_model.encoder.layers.4.self_attn.v_proj.bias: sum: 16.8205  dtype: Float16 shape: [1024]
289: text_model.encoder.layers.4.self_attn.v_proj.weight: sum: 473.3972  dtype: Float16 shape: [1024,1024]
290: text_model.encoder.layers.5.layer_norm1.bias: sum: 19.1656  dtype: Float16 shape: [1024]
291: text_model.encoder.layers.5.layer_norm1.weight: sum: 21.2619  dtype: Float16 shape: [1024]
292: text_model.encoder.layers.5.layer_norm2.bias: sum: 17.7557  dtype: Float16 shape: [1024]
293: text_model.encoder.layers.5.layer_norm2.weight: sum: 21.3677  dtype: Float16 shape: [1024]
294: text_model.encoder.layers.5.mlp.fc1.bias: sum: 42.7128  dtype: Float16 shape: [4096]
295: text_model.encoder.layers.5.mlp.fc1.weight: sum: 1364.4077  dtype: Float16 shape: [4096,1024]
296: text_model.encoder.layers.5.mlp.fc2.bias: sum: 49.945  dtype: Float16 shape: [1024]
297: text_model.encoder.layers.5.mlp.fc2.weight: sum: 3098.6628  dtype: Float16 shape: [1024,4096]
298: text_model.encoder.layers.5.self_attn.k_proj.bias: sum: 25.7275  dtype: Float16 shape: [1024]
299: text_model.encoder.layers.5.self_attn.k_proj.weight: sum: -863.1619  dtype: Float16 shape: [1024,1024]
300: text_model.encoder.layers.5.self_attn.out_proj.bias: sum: 97.9205  dtype: Float16 shape: [1024]
301: text_model.encoder.layers.5.self_attn.out_proj.weight: sum: 2285.5107  dtype: Float16 shape: [1024,1024]
302: text_model.encoder.layers.5.self_attn.q_proj.bias: sum: 19.0196  dtype: Float16 shape: [1024]
303: text_model.encoder.layers.5.self_attn.q_proj.weight: sum: 468.3724  dtype: Float16 shape: [1024,1024]
304: text_model.encoder.layers.5.self_attn.v_proj.bias: sum: 28.2121  dtype: Float16 shape: [1024]
305: text_model.encoder.layers.5.self_attn.v_proj.weight: sum: -718.9138  dtype: Float16 shape: [1024,1024]
306: text_model.encoder.layers.6.layer_norm1.bias: sum: 20.0768  dtype: Float16 shape: [1024]
307: text_model.encoder.layers.6.layer_norm1.weight: sum: 21.2829  dtype: Float16 shape: [1024]
308: text_model.encoder.layers.6.layer_norm2.bias: sum: -6.6624  dtype: Float16 shape: [1024]
309: text_model.encoder.layers.6.layer_norm2.weight: sum: 21.3614  dtype: Float16 shape: [1024]
310: text_model.encoder.layers.6.mlp.fc1.bias: sum: 42.6852  dtype: Float16 shape: [4096]
311: text_model.encoder.layers.6.mlp.fc1.weight: sum: 1397.083  dtype: Float16 shape: [4096,1024]
312: text_model.encoder.layers.6.mlp.fc2.bias: sum: 10.1769  dtype: Float16 shape: [1024]
313: text_model.encoder.layers.6.mlp.fc2.weight: sum: 9218.741  dtype: Float16 shape: [1024,4096]
314: text_model.encoder.layers.6.self_attn.k_proj.bias: sum: 19.675  dtype: Float16 shape: [1024]
315: text_model.encoder.layers.6.self_attn.k_proj.weight: sum: 2417.0137  dtype: Float16 shape: [1024,1024]
316: text_model.encoder.layers.6.self_attn.out_proj.bias: sum: -61.6484  dtype: Float16 shape: [1024]
317: text_model.encoder.layers.6.self_attn.out_proj.weight: sum: 2295.7764  dtype: Float16 shape: [1024,1024]
318: text_model.encoder.layers.6.self_attn.q_proj.bias: sum: 38.2606  dtype: Float16 shape: [1024]
319: text_model.encoder.layers.6.self_attn.q_proj.weight: sum: 5148.11  dtype: Float16 shape: [1024,1024]
320: text_model.encoder.layers.6.self_attn.v_proj.bias: sum: 22.0671  dtype: Float16 shape: [1024]
321: text_model.encoder.layers.6.self_attn.v_proj.weight: sum: 268.7816  dtype: Float16 shape: [1024,1024]
322: text_model.encoder.layers.7.layer_norm1.bias: sum: 19.4611  dtype: Float16 shape: [1024]
323: text_model.encoder.layers.7.layer_norm1.weight: sum: 21.2832  dtype: Float16 shape: [1024]
324: text_model.encoder.layers.7.layer_norm2.bias: sum: 14.7287  dtype: Float16 shape: [1024]
325: text_model.encoder.layers.7.layer_norm2.weight: sum: 21.3495  dtype: Float16 shape: [1024]
326: text_model.encoder.layers.7.mlp.fc1.bias: sum: 42.6398  dtype: Float16 shape: [4096]
327: text_model.encoder.layers.7.mlp.fc1.weight: sum: 1311.739  dtype: Float16 shape: [4096,1024]
328: text_model.encoder.layers.7.mlp.fc2.bias: sum: 43.7017  dtype: Float16 shape: [1024]
329: text_model.encoder.layers.7.mlp.fc2.weight: sum: 2579.3157  dtype: Float16 shape: [1024,4096]
330: text_model.encoder.layers.7.self_attn.k_proj.bias: sum: 15.2544  dtype: Float16 shape: [1024]
331: text_model.encoder.layers.7.self_attn.k_proj.weight: sum: 668.3925  dtype: Float16 shape: [1024,1024]
332: text_model.encoder.layers.7.self_attn.out_proj.bias: sum: 40.6877  dtype: Float16 shape: [1024]
333: text_model.encoder.layers.7.self_attn.out_proj.weight: sum: 492.8246  dtype: Float16 shape: [1024,1024]
334: text_model.encoder.layers.7.self_attn.q_proj.bias: sum: 19.5547  dtype: Float16 shape: [1024]
335: text_model.encoder.layers.7.self_attn.q_proj.weight: sum: 620.3879  dtype: Float16 shape: [1024,1024]
336: text_model.encoder.layers.7.self_attn.v_proj.bias: sum: 16.1021  dtype: Float16 shape: [1024]
337: text_model.encoder.layers.7.self_attn.v_proj.weight: sum: 380.4077  dtype: Float16 shape: [1024,1024]
338: text_model.encoder.layers.8.layer_norm1.bias: sum: 19.6079  dtype: Float16 shape: [1024]
339: text_model.encoder.layers.8.layer_norm1.weight: sum: 21.3083  dtype: Float16 shape: [1024]
340: text_model.encoder.layers.8.layer_norm2.bias: sum: 15.2613  dtype: Float16 shape: [1024]
341: text_model.encoder.layers.8.layer_norm2.weight: sum: 21.3305  dtype: Float16 shape: [1024]
342: text_model.encoder.layers.8.mlp.fc1.bias: sum: 42.6867  dtype: Float16 shape: [4096]
343: text_model.encoder.layers.8.mlp.fc1.weight: sum: 1340.1494  dtype: Float16 shape: [4096,1024]
344: text_model.encoder.layers.8.mlp.fc2.bias: sum: 20.243  dtype: Float16 shape: [1024]
345: text_model.encoder.layers.8.mlp.fc2.weight: sum: 2453.482  dtype: Float16 shape: [1024,4096]
346: text_model.encoder.layers.8.self_attn.k_proj.bias: sum: 6.9028  dtype: Float16 shape: [1024]
347: text_model.encoder.layers.8.self_attn.k_proj.weight: sum: 952.2003  dtype: Float16 shape: [1024,1024]
348: text_model.encoder.layers.8.self_attn.out_proj.bias: sum: 1.9328  dtype: Float16 shape: [1024]
349: text_model.encoder.layers.8.self_attn.out_proj.weight: sum: 3715.3333  dtype: Float16 shape: [1024,1024]
350: text_model.encoder.layers.8.self_attn.q_proj.bias: sum: 27.531  dtype: Float16 shape: [1024]
351: text_model.encoder.layers.8.self_attn.q_proj.weight: sum: 1054.9182  dtype: Float16 shape: [1024,1024]
352: text_model.encoder.layers.8.self_attn.v_proj.bias: sum: 10.4066  dtype: Float16 shape: [1024]
353: text_model.encoder.layers.8.self_attn.v_proj.weight: sum: -1746.8956  dtype: Float16 shape: [1024,1024]
354: text_model.encoder.layers.9.layer_norm1.bias: sum: 19.5285  dtype: Float16 shape: [1024]
355: text_model.encoder.layers.9.layer_norm1.weight: sum: 21.3013  dtype: Float16 shape: [1024]
356: text_model.encoder.layers.9.layer_norm2.bias: sum: 41.9603  dtype: Float16 shape: [1024]
357: text_model.encoder.layers.9.layer_norm2.weight: sum: 21.3263  dtype: Float16 shape: [1024]
358: text_model.encoder.layers.9.mlp.fc1.bias: sum: 42.6597  dtype: Float16 shape: [4096]
359: text_model.encoder.layers.9.mlp.fc1.weight: sum: 1355.931  dtype: Float16 shape: [4096,1024]
360: text_model.encoder.layers.9.mlp.fc2.bias: sum: 28.8163  dtype: Float16 shape: [1024]
361: text_model.encoder.layers.9.mlp.fc2.weight: sum: 3489.732  dtype: Float16 shape: [1024,4096]
362: text_model.encoder.layers.9.self_attn.k_proj.bias: sum: 15.5956  dtype: Float16 shape: [1024]
363: text_model.encoder.layers.9.self_attn.k_proj.weight: sum: 10630.851  dtype: Float16 shape: [1024,1024]
364: text_model.encoder.layers.9.self_attn.out_proj.bias: sum: 54.5595  dtype: Float16 shape: [1024]
365: text_model.encoder.layers.9.self_attn.out_proj.weight: sum: -8557.078  dtype: Float16 shape: [1024,1024]
366: text_model.encoder.layers.9.self_attn.q_proj.bias: sum: 22.7638  dtype: Float16 shape: [1024]
367: text_model.encoder.layers.9.self_attn.q_proj.weight: sum: 685.3954  dtype: Float16 shape: [1024,1024]
368: text_model.encoder.layers.9.self_attn.v_proj.bias: sum: 33.9864  dtype: Float16 shape: [1024]
369: text_model.encoder.layers.9.self_attn.v_proj.weight: sum: 627.0484  dtype: Float16 shape: [1024,1024]
370: text_model.final_layer_norm.bias: sum: 21.4924  dtype: Float16 shape: [1024]
371: text_model.final_layer_norm.weight: sum: 21.3275  dtype: Float16 shape: [1024]
