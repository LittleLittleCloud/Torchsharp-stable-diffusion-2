0: text_model.embeddings.position_embedding.weight: sum: 203.1882  dtype: Float32 shape: [77,1024]
1: text_model.embeddings.token_embedding.weight: sum: -1187.991  dtype: Float32 shape: [49408,1024]
2: text_model.encoder.layers.0.layer_norm1.bias: sum: 17.8894  dtype: Float32 shape: [1024]
3: text_model.encoder.layers.0.layer_norm1.weight: sum: 21.3748  dtype: Float32 shape: [1024]
4: text_model.encoder.layers.0.layer_norm2.bias: sum: 23.9123  dtype: Float32 shape: [1024]
5: text_model.encoder.layers.0.layer_norm2.weight: sum: 21.3078  dtype: Float32 shape: [1024]
6: text_model.encoder.layers.0.mlp.fc1.bias: sum: 42.7505  dtype: Float32 shape: [4096]
7: text_model.encoder.layers.0.mlp.fc1.weight: sum: 1383.503  dtype: Float32 shape: [4096,1024]
8: text_model.encoder.layers.0.mlp.fc2.bias: sum: 21.8875  dtype: Float32 shape: [1024]
9: text_model.encoder.layers.0.mlp.fc2.weight: sum: 3668.4844  dtype: Float32 shape: [1024,4096]
10: text_model.encoder.layers.0.self_attn.k_proj.bias: sum: 34.0243  dtype: Float32 shape: [1024]
11: text_model.encoder.layers.0.self_attn.k_proj.weight: sum: 810.41  dtype: Float32 shape: [1024,1024]
12: text_model.encoder.layers.0.self_attn.out_proj.bias: sum: 14.8221  dtype: Float32 shape: [1024]
13: text_model.encoder.layers.0.self_attn.out_proj.weight: sum: 1112.7013  dtype: Float32 shape: [1024,1024]
14: text_model.encoder.layers.0.self_attn.q_proj.bias: sum: 22.4455  dtype: Float32 shape: [1024]
15: text_model.encoder.layers.0.self_attn.q_proj.weight: sum: 650.831  dtype: Float32 shape: [1024,1024]
16: text_model.encoder.layers.0.self_attn.v_proj.bias: sum: 602.234  dtype: Float32 shape: [1024]
17: text_model.encoder.layers.0.self_attn.v_proj.weight: sum: 818.9112  dtype: Float32 shape: [1024,1024]
18: text_model.encoder.layers.1.layer_norm1.bias: sum: 19.2233  dtype: Float32 shape: [1024]
19: text_model.encoder.layers.1.layer_norm1.weight: sum: 21.2818  dtype: Float32 shape: [1024]
20: text_model.encoder.layers.1.layer_norm2.bias: sum: 19.5212  dtype: Float32 shape: [1024]
21: text_model.encoder.layers.1.layer_norm2.weight: sum: 21.3747  dtype: Float32 shape: [1024]
22: text_model.encoder.layers.1.mlp.fc1.bias: sum: 42.7113  dtype: Float32 shape: [4096]
23: text_model.encoder.layers.1.mlp.fc1.weight: sum: 1373.5457  dtype: Float32 shape: [4096,1024]
24: text_model.encoder.layers.1.mlp.fc2.bias: sum: 26.8014  dtype: Float32 shape: [1024]
25: text_model.encoder.layers.1.mlp.fc2.weight: sum: 5114.397  dtype: Float32 shape: [1024,4096]
26: text_model.encoder.layers.1.self_attn.k_proj.bias: sum: 27.9194  dtype: Float32 shape: [1024]
27: text_model.encoder.layers.1.self_attn.k_proj.weight: sum: 452.3188  dtype: Float32 shape: [1024,1024]
28: text_model.encoder.layers.1.self_attn.out_proj.bias: sum: 28.5062  dtype: Float32 shape: [1024]
29: text_model.encoder.layers.1.self_attn.out_proj.weight: sum: 1529.9224  dtype: Float32 shape: [1024,1024]
30: text_model.encoder.layers.1.self_attn.q_proj.bias: sum: 26.2778  dtype: Float32 shape: [1024]
31: text_model.encoder.layers.1.self_attn.q_proj.weight: sum: 722.2649  dtype: Float32 shape: [1024,1024]
32: text_model.encoder.layers.1.self_attn.v_proj.bias: sum: 58.0835  dtype: Float32 shape: [1024]
33: text_model.encoder.layers.1.self_attn.v_proj.weight: sum: 1004.397  dtype: Float32 shape: [1024,1024]
34: text_model.encoder.layers.10.layer_norm1.bias: sum: 19.793  dtype: Float32 shape: [1024]
35: text_model.encoder.layers.10.layer_norm1.weight: sum: 21.2937  dtype: Float32 shape: [1024]
36: text_model.encoder.layers.10.layer_norm2.bias: sum: -19.2374  dtype: Float32 shape: [1024]
37: text_model.encoder.layers.10.layer_norm2.weight: sum: 21.3171  dtype: Float32 shape: [1024]
38: text_model.encoder.layers.10.mlp.fc1.bias: sum: 42.6565  dtype: Float32 shape: [4096]
39: text_model.encoder.layers.10.mlp.fc1.weight: sum: 1485.4861  dtype: Float32 shape: [4096,1024]
40: text_model.encoder.layers.10.mlp.fc2.bias: sum: 3.0549  dtype: Float32 shape: [1024]
41: text_model.encoder.layers.10.mlp.fc2.weight: sum: 436.8454  dtype: Float32 shape: [1024,4096]
42: text_model.encoder.layers.10.self_attn.k_proj.bias: sum: 10.1457  dtype: Float32 shape: [1024]
43: text_model.encoder.layers.10.self_attn.k_proj.weight: sum: -335.4892  dtype: Float32 shape: [1024,1024]
44: text_model.encoder.layers.10.self_attn.out_proj.bias: sum: 30.2472  dtype: Float32 shape: [1024]
45: text_model.encoder.layers.10.self_attn.out_proj.weight: sum: 2243.202  dtype: Float32 shape: [1024,1024]
46: text_model.encoder.layers.10.self_attn.q_proj.bias: sum: 167.7943  dtype: Float32 shape: [1024]
47: text_model.encoder.layers.10.self_attn.q_proj.weight: sum: 263.5481  dtype: Float32 shape: [1024,1024]
48: text_model.encoder.layers.10.self_attn.v_proj.bias: sum: 34.5645  dtype: Float32 shape: [1024]
49: text_model.encoder.layers.10.self_attn.v_proj.weight: sum: 557.2146  dtype: Float32 shape: [1024,1024]
50: text_model.encoder.layers.11.layer_norm1.bias: sum: 19.8627  dtype: Float32 shape: [1024]
51: text_model.encoder.layers.11.layer_norm1.weight: sum: 21.3166  dtype: Float32 shape: [1024]
52: text_model.encoder.layers.11.layer_norm2.bias: sum: 18.6239  dtype: Float32 shape: [1024]
53: text_model.encoder.layers.11.layer_norm2.weight: sum: 21.3324  dtype: Float32 shape: [1024]
54: text_model.encoder.layers.11.mlp.fc1.bias: sum: 42.7319  dtype: Float32 shape: [4096]
55: text_model.encoder.layers.11.mlp.fc1.weight: sum: 1364.695  dtype: Float32 shape: [4096,1024]
56: text_model.encoder.layers.11.mlp.fc2.bias: sum: 11.8028  dtype: Float32 shape: [1024]
57: text_model.encoder.layers.11.mlp.fc2.weight: sum: 674.5792  dtype: Float32 shape: [1024,4096]
58: text_model.encoder.layers.11.self_attn.k_proj.bias: sum: 49.4092  dtype: Float32 shape: [1024]
59: text_model.encoder.layers.11.self_attn.k_proj.weight: sum: 547.7692  dtype: Float32 shape: [1024,1024]
60: text_model.encoder.layers.11.self_attn.out_proj.bias: sum: 45.9598  dtype: Float32 shape: [1024]
61: text_model.encoder.layers.11.self_attn.out_proj.weight: sum: 133.0811  dtype: Float32 shape: [1024,1024]
62: text_model.encoder.layers.11.self_attn.q_proj.bias: sum: 16.7404  dtype: Float32 shape: [1024]
63: text_model.encoder.layers.11.self_attn.q_proj.weight: sum: -474.3118  dtype: Float32 shape: [1024,1024]
64: text_model.encoder.layers.11.self_attn.v_proj.bias: sum: 40.6992  dtype: Float32 shape: [1024]
65: text_model.encoder.layers.11.self_attn.v_proj.weight: sum: 809.8759  dtype: Float32 shape: [1024,1024]
66: text_model.encoder.layers.12.layer_norm1.bias: sum: 19.6832  dtype: Float32 shape: [1024]
67: text_model.encoder.layers.12.layer_norm1.weight: sum: 21.3259  dtype: Float32 shape: [1024]
68: text_model.encoder.layers.12.layer_norm2.bias: sum: 27.8546  dtype: Float32 shape: [1024]
69: text_model.encoder.layers.12.layer_norm2.weight: sum: 21.3375  dtype: Float32 shape: [1024]
70: text_model.encoder.layers.12.mlp.fc1.bias: sum: 42.7257  dtype: Float32 shape: [4096]
71: text_model.encoder.layers.12.mlp.fc1.weight: sum: 1230.0759  dtype: Float32 shape: [4096,1024]
72: text_model.encoder.layers.12.mlp.fc2.bias: sum: 29.3628  dtype: Float32 shape: [1024]
73: text_model.encoder.layers.12.mlp.fc2.weight: sum: 2304.3306  dtype: Float32 shape: [1024,4096]
74: text_model.encoder.layers.12.self_attn.k_proj.bias: sum: 74.1095  dtype: Float32 shape: [1024]
75: text_model.encoder.layers.12.self_attn.k_proj.weight: sum: 712.0804  dtype: Float32 shape: [1024,1024]
76: text_model.encoder.layers.12.self_attn.out_proj.bias: sum: 17.5474  dtype: Float32 shape: [1024]
77: text_model.encoder.layers.12.self_attn.out_proj.weight: sum: 201.2653  dtype: Float32 shape: [1024,1024]
78: text_model.encoder.layers.12.self_attn.q_proj.bias: sum: 2.0188  dtype: Float32 shape: [1024]
79: text_model.encoder.layers.12.self_attn.q_proj.weight: sum: 613.2554  dtype: Float32 shape: [1024,1024]
80: text_model.encoder.layers.12.self_attn.v_proj.bias: sum: -11.5242  dtype: Float32 shape: [1024]
81: text_model.encoder.layers.12.self_attn.v_proj.weight: sum: 384.0118  dtype: Float32 shape: [1024,1024]
82: text_model.encoder.layers.13.layer_norm1.bias: sum: 19.6742  dtype: Float32 shape: [1024]
83: text_model.encoder.layers.13.layer_norm1.weight: sum: 21.3213  dtype: Float32 shape: [1024]
84: text_model.encoder.layers.13.layer_norm2.bias: sum: 24.5342  dtype: Float32 shape: [1024]
85: text_model.encoder.layers.13.layer_norm2.weight: sum: 21.3403  dtype: Float32 shape: [1024]
86: text_model.encoder.layers.13.mlp.fc1.bias: sum: 42.5811  dtype: Float32 shape: [4096]
87: text_model.encoder.layers.13.mlp.fc1.weight: sum: 1365.5549  dtype: Float32 shape: [4096,1024]
88: text_model.encoder.layers.13.mlp.fc2.bias: sum: 21.4501  dtype: Float32 shape: [1024]
89: text_model.encoder.layers.13.mlp.fc2.weight: sum: -723.5659  dtype: Float32 shape: [1024,4096]
90: text_model.encoder.layers.13.self_attn.k_proj.bias: sum: 20.8238  dtype: Float32 shape: [1024]
91: text_model.encoder.layers.13.self_attn.k_proj.weight: sum: 618.4666  dtype: Float32 shape: [1024,1024]
92: text_model.encoder.layers.13.self_attn.out_proj.bias: sum: 18.8135  dtype: Float32 shape: [1024]
93: text_model.encoder.layers.13.self_attn.out_proj.weight: sum: 3772.4832  dtype: Float32 shape: [1024,1024]
94: text_model.encoder.layers.13.self_attn.q_proj.bias: sum: 19.938  dtype: Float32 shape: [1024]
95: text_model.encoder.layers.13.self_attn.q_proj.weight: sum: 657.0204  dtype: Float32 shape: [1024,1024]
96: text_model.encoder.layers.13.self_attn.v_proj.bias: sum: 12.6465  dtype: Float32 shape: [1024]
97: text_model.encoder.layers.13.self_attn.v_proj.weight: sum: 797.5182  dtype: Float32 shape: [1024,1024]
98: text_model.encoder.layers.14.layer_norm1.bias: sum: 19.5652  dtype: Float32 shape: [1024]
99: text_model.encoder.layers.14.layer_norm1.weight: sum: 21.3316  dtype: Float32 shape: [1024]
100: text_model.encoder.layers.14.layer_norm2.bias: sum: 24.9479  dtype: Float32 shape: [1024]
101: text_model.encoder.layers.14.layer_norm2.weight: sum: 21.3487  dtype: Float32 shape: [1024]
102: text_model.encoder.layers.14.mlp.fc1.bias: sum: 42.5596  dtype: Float32 shape: [4096]
103: text_model.encoder.layers.14.mlp.fc1.weight: sum: 1364.2949  dtype: Float32 shape: [4096,1024]
104: text_model.encoder.layers.14.mlp.fc2.bias: sum: 22.9154  dtype: Float32 shape: [1024]
105: text_model.encoder.layers.14.mlp.fc2.weight: sum: 256.7331  dtype: Float32 shape: [1024,4096]
106: text_model.encoder.layers.14.self_attn.k_proj.bias: sum: 17.8848  dtype: Float32 shape: [1024]
107: text_model.encoder.layers.14.self_attn.k_proj.weight: sum: 321.1429  dtype: Float32 shape: [1024,1024]
108: text_model.encoder.layers.14.self_attn.out_proj.bias: sum: 101.8686  dtype: Float32 shape: [1024]
109: text_model.encoder.layers.14.self_attn.out_proj.weight: sum: 21305.64  dtype: Float32 shape: [1024,1024]
110: text_model.encoder.layers.14.self_attn.q_proj.bias: sum: 11.4487  dtype: Float32 shape: [1024]
111: text_model.encoder.layers.14.self_attn.q_proj.weight: sum: 651.593  dtype: Float32 shape: [1024,1024]
112: text_model.encoder.layers.14.self_attn.v_proj.bias: sum: 26.8916  dtype: Float32 shape: [1024]
113: text_model.encoder.layers.14.self_attn.v_proj.weight: sum: 774.7245  dtype: Float32 shape: [1024,1024]
114: text_model.encoder.layers.15.layer_norm1.bias: sum: 19.314  dtype: Float32 shape: [1024]
115: text_model.encoder.layers.15.layer_norm1.weight: sum: 21.3341  dtype: Float32 shape: [1024]
116: text_model.encoder.layers.15.layer_norm2.bias: sum: 30.1124  dtype: Float32 shape: [1024]
117: text_model.encoder.layers.15.layer_norm2.weight: sum: 21.3233  dtype: Float32 shape: [1024]
118: text_model.encoder.layers.15.mlp.fc1.bias: sum: 42.6541  dtype: Float32 shape: [4096]
119: text_model.encoder.layers.15.mlp.fc1.weight: sum: 1363.5001  dtype: Float32 shape: [4096,1024]
120: text_model.encoder.layers.15.mlp.fc2.bias: sum: 50.3215  dtype: Float32 shape: [1024]
121: text_model.encoder.layers.15.mlp.fc2.weight: sum: 1495.5734  dtype: Float32 shape: [1024,4096]
122: text_model.encoder.layers.15.self_attn.k_proj.bias: sum: 33.2143  dtype: Float32 shape: [1024]
123: text_model.encoder.layers.15.self_attn.k_proj.weight: sum: 938.695  dtype: Float32 shape: [1024,1024]
124: text_model.encoder.layers.15.self_attn.out_proj.bias: sum: 19.84  dtype: Float32 shape: [1024]
125: text_model.encoder.layers.15.self_attn.out_proj.weight: sum: 2159.671  dtype: Float32 shape: [1024,1024]
126: text_model.encoder.layers.15.self_attn.q_proj.bias: sum: 30.6304  dtype: Float32 shape: [1024]
127: text_model.encoder.layers.15.self_attn.q_proj.weight: sum: 866.8048  dtype: Float32 shape: [1024,1024]
128: text_model.encoder.layers.15.self_attn.v_proj.bias: sum: 15.0762  dtype: Float32 shape: [1024]
129: text_model.encoder.layers.15.self_attn.v_proj.weight: sum: 959.7522  dtype: Float32 shape: [1024,1024]
130: text_model.encoder.layers.16.layer_norm1.bias: sum: 19.6948  dtype: Float32 shape: [1024]
131: text_model.encoder.layers.16.layer_norm1.weight: sum: 21.3161  dtype: Float32 shape: [1024]
132: text_model.encoder.layers.16.layer_norm2.bias: sum: 23.7899  dtype: Float32 shape: [1024]
133: text_model.encoder.layers.16.layer_norm2.weight: sum: 21.3429  dtype: Float32 shape: [1024]
134: text_model.encoder.layers.16.mlp.fc1.bias: sum: 42.5845  dtype: Float32 shape: [4096]
135: text_model.encoder.layers.16.mlp.fc1.weight: sum: 1369.341  dtype: Float32 shape: [4096,1024]
136: text_model.encoder.layers.16.mlp.fc2.bias: sum: 56.0605  dtype: Float32 shape: [1024]
137: text_model.encoder.layers.16.mlp.fc2.weight: sum: 1079.6504  dtype: Float32 shape: [1024,4096]
138: text_model.encoder.layers.16.self_attn.k_proj.bias: sum: -131.5363  dtype: Float32 shape: [1024]
139: text_model.encoder.layers.16.self_attn.k_proj.weight: sum: 879.056  dtype: Float32 shape: [1024,1024]
140: text_model.encoder.layers.16.self_attn.out_proj.bias: sum: 41.3958  dtype: Float32 shape: [1024]
141: text_model.encoder.layers.16.self_attn.out_proj.weight: sum: 1087.9045  dtype: Float32 shape: [1024,1024]
142: text_model.encoder.layers.16.self_attn.q_proj.bias: sum: 39.902  dtype: Float32 shape: [1024]
143: text_model.encoder.layers.16.self_attn.q_proj.weight: sum: -151.6412  dtype: Float32 shape: [1024,1024]
144: text_model.encoder.layers.16.self_attn.v_proj.bias: sum: 25.6334  dtype: Float32 shape: [1024]
145: text_model.encoder.layers.16.self_attn.v_proj.weight: sum: 1085.8488  dtype: Float32 shape: [1024,1024]
146: text_model.encoder.layers.17.layer_norm1.bias: sum: 19.2446  dtype: Float32 shape: [1024]
147: text_model.encoder.layers.17.layer_norm1.weight: sum: 21.3173  dtype: Float32 shape: [1024]
148: text_model.encoder.layers.17.layer_norm2.bias: sum: 23.3434  dtype: Float32 shape: [1024]
149: text_model.encoder.layers.17.layer_norm2.weight: sum: 21.3283  dtype: Float32 shape: [1024]
150: text_model.encoder.layers.17.mlp.fc1.bias: sum: 42.6946  dtype: Float32 shape: [4096]
151: text_model.encoder.layers.17.mlp.fc1.weight: sum: 1368.962  dtype: Float32 shape: [4096,1024]
152: text_model.encoder.layers.17.mlp.fc2.bias: sum: 3.6329  dtype: Float32 shape: [1024]
153: text_model.encoder.layers.17.mlp.fc2.weight: sum: 1505.2396  dtype: Float32 shape: [1024,4096]
154: text_model.encoder.layers.17.self_attn.k_proj.bias: sum: 9.7198  dtype: Float32 shape: [1024]
155: text_model.encoder.layers.17.self_attn.k_proj.weight: sum: 327.0394  dtype: Float32 shape: [1024,1024]
156: text_model.encoder.layers.17.self_attn.out_proj.bias: sum: 16.6657  dtype: Float32 shape: [1024]
157: text_model.encoder.layers.17.self_attn.out_proj.weight: sum: 16439.828  dtype: Float32 shape: [1024,1024]
158: text_model.encoder.layers.17.self_attn.q_proj.bias: sum: 31.0063  dtype: Float32 shape: [1024]
159: text_model.encoder.layers.17.self_attn.q_proj.weight: sum: 1092.2161  dtype: Float32 shape: [1024,1024]
160: text_model.encoder.layers.17.self_attn.v_proj.bias: sum: 16.0547  dtype: Float32 shape: [1024]
161: text_model.encoder.layers.17.self_attn.v_proj.weight: sum: 705.5674  dtype: Float32 shape: [1024,1024]
162: text_model.encoder.layers.18.layer_norm1.bias: sum: 19.0427  dtype: Float32 shape: [1024]
163: text_model.encoder.layers.18.layer_norm1.weight: sum: 21.3269  dtype: Float32 shape: [1024]
164: text_model.encoder.layers.18.layer_norm2.bias: sum: 23.1434  dtype: Float32 shape: [1024]
165: text_model.encoder.layers.18.layer_norm2.weight: sum: 21.3326  dtype: Float32 shape: [1024]
166: text_model.encoder.layers.18.mlp.fc1.bias: sum: 42.6487  dtype: Float32 shape: [4096]
167: text_model.encoder.layers.18.mlp.fc1.weight: sum: 1366.7054  dtype: Float32 shape: [4096,1024]
168: text_model.encoder.layers.18.mlp.fc2.bias: sum: -24.8053  dtype: Float32 shape: [1024]
169: text_model.encoder.layers.18.mlp.fc2.weight: sum: 2265.3372  dtype: Float32 shape: [1024,4096]
170: text_model.encoder.layers.18.self_attn.k_proj.bias: sum: 23.2704  dtype: Float32 shape: [1024]
171: text_model.encoder.layers.18.self_attn.k_proj.weight: sum: 796.4293  dtype: Float32 shape: [1024,1024]
172: text_model.encoder.layers.18.self_attn.out_proj.bias: sum: 30.9912  dtype: Float32 shape: [1024]
173: text_model.encoder.layers.18.self_attn.out_proj.weight: sum: -5705.7256  dtype: Float32 shape: [1024,1024]
174: text_model.encoder.layers.18.self_attn.q_proj.bias: sum: 24.2637  dtype: Float32 shape: [1024]
175: text_model.encoder.layers.18.self_attn.q_proj.weight: sum: -3220.0989  dtype: Float32 shape: [1024,1024]
176: text_model.encoder.layers.18.self_attn.v_proj.bias: sum: 31.6533  dtype: Float32 shape: [1024]
177: text_model.encoder.layers.18.self_attn.v_proj.weight: sum: 850.3905  dtype: Float32 shape: [1024,1024]
178: text_model.encoder.layers.19.layer_norm1.bias: sum: 19.1249  dtype: Float32 shape: [1024]
179: text_model.encoder.layers.19.layer_norm1.weight: sum: 21.3262  dtype: Float32 shape: [1024]
180: text_model.encoder.layers.19.layer_norm2.bias: sum: 20.9845  dtype: Float32 shape: [1024]
181: text_model.encoder.layers.19.layer_norm2.weight: sum: 21.321  dtype: Float32 shape: [1024]
182: text_model.encoder.layers.19.mlp.fc1.bias: sum: 42.6838  dtype: Float32 shape: [4096]
183: text_model.encoder.layers.19.mlp.fc1.weight: sum: 1362.6721  dtype: Float32 shape: [4096,1024]
184: text_model.encoder.layers.19.mlp.fc2.bias: sum: -212.466  dtype: Float32 shape: [1024]
185: text_model.encoder.layers.19.mlp.fc2.weight: sum: 3234.8135  dtype: Float32 shape: [1024,4096]
186: text_model.encoder.layers.19.self_attn.k_proj.bias: sum: 14.1222  dtype: Float32 shape: [1024]
187: text_model.encoder.layers.19.self_attn.k_proj.weight: sum: 400.6696  dtype: Float32 shape: [1024,1024]
188: text_model.encoder.layers.19.self_attn.out_proj.bias: sum: 42.8977  dtype: Float32 shape: [1024]
189: text_model.encoder.layers.19.self_attn.out_proj.weight: sum: 373.4884  dtype: Float32 shape: [1024,1024]
190: text_model.encoder.layers.19.self_attn.q_proj.bias: sum: 16.8597  dtype: Float32 shape: [1024]
191: text_model.encoder.layers.19.self_attn.q_proj.weight: sum: 339.8519  dtype: Float32 shape: [1024,1024]
192: text_model.encoder.layers.19.self_attn.v_proj.bias: sum: 37.949  dtype: Float32 shape: [1024]
193: text_model.encoder.layers.19.self_attn.v_proj.weight: sum: -4705.634  dtype: Float32 shape: [1024,1024]
194: text_model.encoder.layers.2.layer_norm1.bias: sum: 19.1232  dtype: Float32 shape: [1024]
195: text_model.encoder.layers.2.layer_norm1.weight: sum: 21.2537  dtype: Float32 shape: [1024]
196: text_model.encoder.layers.2.layer_norm2.bias: sum: 24.745  dtype: Float32 shape: [1024]
197: text_model.encoder.layers.2.layer_norm2.weight: sum: 21.3351  dtype: Float32 shape: [1024]
198: text_model.encoder.layers.2.mlp.fc1.bias: sum: 42.7779  dtype: Float32 shape: [4096]
199: text_model.encoder.layers.2.mlp.fc1.weight: sum: 1362.0442  dtype: Float32 shape: [4096,1024]
200: text_model.encoder.layers.2.mlp.fc2.bias: sum: 25.5313  dtype: Float32 shape: [1024]
201: text_model.encoder.layers.2.mlp.fc2.weight: sum: 11812.949  dtype: Float32 shape: [1024,4096]
202: text_model.encoder.layers.2.self_attn.k_proj.bias: sum: 20.4399  dtype: Float32 shape: [1024]
203: text_model.encoder.layers.2.self_attn.k_proj.weight: sum: 1011.6801  dtype: Float32 shape: [1024,1024]
204: text_model.encoder.layers.2.self_attn.out_proj.bias: sum: 25.0175  dtype: Float32 shape: [1024]
205: text_model.encoder.layers.2.self_attn.out_proj.weight: sum: -3318.1802  dtype: Float32 shape: [1024,1024]
206: text_model.encoder.layers.2.self_attn.q_proj.bias: sum: 27.7403  dtype: Float32 shape: [1024]
207: text_model.encoder.layers.2.self_attn.q_proj.weight: sum: -6949.224  dtype: Float32 shape: [1024,1024]
208: text_model.encoder.layers.2.self_attn.v_proj.bias: sum: 26.0591  dtype: Float32 shape: [1024]
209: text_model.encoder.layers.2.self_attn.v_proj.weight: sum: 1449.5626  dtype: Float32 shape: [1024,1024]
210: text_model.encoder.layers.20.layer_norm1.bias: sum: 19.3022  dtype: Float32 shape: [1024]
211: text_model.encoder.layers.20.layer_norm1.weight: sum: 21.3142  dtype: Float32 shape: [1024]
212: text_model.encoder.layers.20.layer_norm2.bias: sum: 20.5431  dtype: Float32 shape: [1024]
213: text_model.encoder.layers.20.layer_norm2.weight: sum: 21.3295  dtype: Float32 shape: [1024]
214: text_model.encoder.layers.20.mlp.fc1.bias: sum: 42.6755  dtype: Float32 shape: [4096]
215: text_model.encoder.layers.20.mlp.fc1.weight: sum: 1364.0837  dtype: Float32 shape: [4096,1024]
216: text_model.encoder.layers.20.mlp.fc2.bias: sum: 96.0005  dtype: Float32 shape: [1024]
217: text_model.encoder.layers.20.mlp.fc2.weight: sum: 2305.31  dtype: Float32 shape: [1024,4096]
218: text_model.encoder.layers.20.self_attn.k_proj.bias: sum: 30.3675  dtype: Float32 shape: [1024]
219: text_model.encoder.layers.20.self_attn.k_proj.weight: sum: 474.0659  dtype: Float32 shape: [1024,1024]
220: text_model.encoder.layers.20.self_attn.out_proj.bias: sum: 25.5401  dtype: Float32 shape: [1024]
221: text_model.encoder.layers.20.self_attn.out_proj.weight: sum: -159943.28  dtype: Float32 shape: [1024,1024]
222: text_model.encoder.layers.20.self_attn.q_proj.bias: sum: 19.59  dtype: Float32 shape: [1024]
223: text_model.encoder.layers.20.self_attn.q_proj.weight: sum: 726.3578  dtype: Float32 shape: [1024,1024]
224: text_model.encoder.layers.20.self_attn.v_proj.bias: sum: 20.686  dtype: Float32 shape: [1024]
225: text_model.encoder.layers.20.self_attn.v_proj.weight: sum: 704.6305  dtype: Float32 shape: [1024,1024]
226: text_model.encoder.layers.21.layer_norm1.bias: sum: 19.3589  dtype: Float32 shape: [1024]
227: text_model.encoder.layers.21.layer_norm1.weight: sum: 21.3049  dtype: Float32 shape: [1024]
228: text_model.encoder.layers.21.layer_norm2.bias: sum: 25.0727  dtype: Float32 shape: [1024]
229: text_model.encoder.layers.21.layer_norm2.weight: sum: 21.3096  dtype: Float32 shape: [1024]
230: text_model.encoder.layers.21.mlp.fc1.bias: sum: 42.6688  dtype: Float32 shape: [4096]
231: text_model.encoder.layers.21.mlp.fc1.weight: sum: 1370.9066  dtype: Float32 shape: [4096,1024]
232: text_model.encoder.layers.21.mlp.fc2.bias: sum: -123.2141  dtype: Float32 shape: [1024]
233: text_model.encoder.layers.21.mlp.fc2.weight: sum: -265.3181  dtype: Float32 shape: [1024,4096]
234: text_model.encoder.layers.21.self_attn.k_proj.bias: sum: 137.829  dtype: Float32 shape: [1024]
235: text_model.encoder.layers.21.self_attn.k_proj.weight: sum: 766.729  dtype: Float32 shape: [1024,1024]
236: text_model.encoder.layers.21.self_attn.out_proj.bias: sum: 31.6015  dtype: Float32 shape: [1024]
237: text_model.encoder.layers.21.self_attn.out_proj.weight: sum: 5032.5176  dtype: Float32 shape: [1024,1024]
238: text_model.encoder.layers.21.self_attn.q_proj.bias: sum: 43.5649  dtype: Float32 shape: [1024]
239: text_model.encoder.layers.21.self_attn.q_proj.weight: sum: -1177.42  dtype: Float32 shape: [1024,1024]
240: text_model.encoder.layers.21.self_attn.v_proj.bias: sum: 25.1726  dtype: Float32 shape: [1024]
241: text_model.encoder.layers.21.self_attn.v_proj.weight: sum: 462.7944  dtype: Float32 shape: [1024,1024]
242: text_model.encoder.layers.22.layer_norm1.bias: sum: 19.122  dtype: Float32 shape: [1024]
243: text_model.encoder.layers.22.layer_norm1.weight: sum: 21.309  dtype: Float32 shape: [1024]
244: text_model.encoder.layers.22.layer_norm2.bias: sum: 22.9296  dtype: Float32 shape: [1024]
245: text_model.encoder.layers.22.layer_norm2.weight: sum: 21.3258  dtype: Float32 shape: [1024]
246: text_model.encoder.layers.22.mlp.fc1.bias: sum: 42.6207  dtype: Float32 shape: [4096]
247: text_model.encoder.layers.22.mlp.fc1.weight: sum: 1367.6178  dtype: Float32 shape: [4096,1024]
248: text_model.encoder.layers.22.mlp.fc2.bias: sum: 11.6078  dtype: Float32 shape: [1024]
249: text_model.encoder.layers.22.mlp.fc2.weight: sum: 882.2838  dtype: Float32 shape: [1024,4096]
250: text_model.encoder.layers.22.self_attn.k_proj.bias: sum: 25.5244  dtype: Float32 shape: [1024]
251: text_model.encoder.layers.22.self_attn.k_proj.weight: sum: 1170.972  dtype: Float32 shape: [1024,1024]
252: text_model.encoder.layers.22.self_attn.out_proj.bias: sum: 249.7495  dtype: Float32 shape: [1024]
253: text_model.encoder.layers.22.self_attn.out_proj.weight: sum: 58.1516  dtype: Float32 shape: [1024,1024]
254: text_model.encoder.layers.22.self_attn.q_proj.bias: sum: 23.8319  dtype: Float32 shape: [1024]
255: text_model.encoder.layers.22.self_attn.q_proj.weight: sum: 925.9888  dtype: Float32 shape: [1024,1024]
256: text_model.encoder.layers.22.self_attn.v_proj.bias: sum: 19.1277  dtype: Float32 shape: [1024]
257: text_model.encoder.layers.22.self_attn.v_proj.weight: sum: 514.8075  dtype: Float32 shape: [1024,1024]
258: text_model.encoder.layers.3.layer_norm1.bias: sum: 19.5002  dtype: Float32 shape: [1024]
259: text_model.encoder.layers.3.layer_norm1.weight: sum: 21.2697  dtype: Float32 shape: [1024]
260: text_model.encoder.layers.3.layer_norm2.bias: sum: 36.8904  dtype: Float32 shape: [1024]
261: text_model.encoder.layers.3.layer_norm2.weight: sum: 21.3464  dtype: Float32 shape: [1024]
262: text_model.encoder.layers.3.mlp.fc1.bias: sum: 42.607  dtype: Float32 shape: [4096]
263: text_model.encoder.layers.3.mlp.fc1.weight: sum: 1358.4598  dtype: Float32 shape: [4096,1024]
264: text_model.encoder.layers.3.mlp.fc2.bias: sum: 13.392  dtype: Float32 shape: [1024]
265: text_model.encoder.layers.3.mlp.fc2.weight: sum: 5255.6323  dtype: Float32 shape: [1024,4096]
266: text_model.encoder.layers.3.self_attn.k_proj.bias: sum: 14.4924  dtype: Float32 shape: [1024]
267: text_model.encoder.layers.3.self_attn.k_proj.weight: sum: 1449.0038  dtype: Float32 shape: [1024,1024]
268: text_model.encoder.layers.3.self_attn.out_proj.bias: sum: 70.1653  dtype: Float32 shape: [1024]
269: text_model.encoder.layers.3.self_attn.out_proj.weight: sum: -795.2328  dtype: Float32 shape: [1024,1024]
270: text_model.encoder.layers.3.self_attn.q_proj.bias: sum: 139.0525  dtype: Float32 shape: [1024]
271: text_model.encoder.layers.3.self_attn.q_proj.weight: sum: 691.0141  dtype: Float32 shape: [1024,1024]
272: text_model.encoder.layers.3.self_attn.v_proj.bias: sum: 24.8237  dtype: Float32 shape: [1024]
273: text_model.encoder.layers.3.self_attn.v_proj.weight: sum: 487.5666  dtype: Float32 shape: [1024,1024]
274: text_model.encoder.layers.4.layer_norm1.bias: sum: 19.1415  dtype: Float32 shape: [1024]
275: text_model.encoder.layers.4.layer_norm1.weight: sum: 21.2658  dtype: Float32 shape: [1024]
276: text_model.encoder.layers.4.layer_norm2.bias: sum: 25.0472  dtype: Float32 shape: [1024]
277: text_model.encoder.layers.4.layer_norm2.weight: sum: 21.3596  dtype: Float32 shape: [1024]
278: text_model.encoder.layers.4.mlp.fc1.bias: sum: 42.6558  dtype: Float32 shape: [4096]
279: text_model.encoder.layers.4.mlp.fc1.weight: sum: 1369.7227  dtype: Float32 shape: [4096,1024]
280: text_model.encoder.layers.4.mlp.fc2.bias: sum: 30.0171  dtype: Float32 shape: [1024]
281: text_model.encoder.layers.4.mlp.fc2.weight: sum: 677.9349  dtype: Float32 shape: [1024,4096]
282: text_model.encoder.layers.4.self_attn.k_proj.bias: sum: 17.8438  dtype: Float32 shape: [1024]
283: text_model.encoder.layers.4.self_attn.k_proj.weight: sum: 888.2687  dtype: Float32 shape: [1024,1024]
284: text_model.encoder.layers.4.self_attn.out_proj.bias: sum: 24.7816  dtype: Float32 shape: [1024]
285: text_model.encoder.layers.4.self_attn.out_proj.weight: sum: -1676.9677  dtype: Float32 shape: [1024,1024]
286: text_model.encoder.layers.4.self_attn.q_proj.bias: sum: 25.8153  dtype: Float32 shape: [1024]
287: text_model.encoder.layers.4.self_attn.q_proj.weight: sum: 604.4666  dtype: Float32 shape: [1024,1024]
288: text_model.encoder.layers.4.self_attn.v_proj.bias: sum: 16.8232  dtype: Float32 shape: [1024]
289: text_model.encoder.layers.4.self_attn.v_proj.weight: sum: 472.9614  dtype: Float32 shape: [1024,1024]
290: text_model.encoder.layers.5.layer_norm1.bias: sum: 19.159  dtype: Float32 shape: [1024]
291: text_model.encoder.layers.5.layer_norm1.weight: sum: 21.2682  dtype: Float32 shape: [1024]
292: text_model.encoder.layers.5.layer_norm2.bias: sum: 17.7528  dtype: Float32 shape: [1024]
293: text_model.encoder.layers.5.layer_norm2.weight: sum: 21.3693  dtype: Float32 shape: [1024]
294: text_model.encoder.layers.5.mlp.fc1.bias: sum: 42.7007  dtype: Float32 shape: [4096]
295: text_model.encoder.layers.5.mlp.fc1.weight: sum: 1364.5253  dtype: Float32 shape: [4096,1024]
296: text_model.encoder.layers.5.mlp.fc2.bias: sum: 49.9988  dtype: Float32 shape: [1024]
297: text_model.encoder.layers.5.mlp.fc2.weight: sum: 3111.4094  dtype: Float32 shape: [1024,4096]
298: text_model.encoder.layers.5.self_attn.k_proj.bias: sum: 25.7222  dtype: Float32 shape: [1024]
299: text_model.encoder.layers.5.self_attn.k_proj.weight: sum: -856.8591  dtype: Float32 shape: [1024,1024]
300: text_model.encoder.layers.5.self_attn.out_proj.bias: sum: 97.8182  dtype: Float32 shape: [1024]
301: text_model.encoder.layers.5.self_attn.out_proj.weight: sum: 2282.656  dtype: Float32 shape: [1024,1024]
302: text_model.encoder.layers.5.self_attn.q_proj.bias: sum: 19.026  dtype: Float32 shape: [1024]
303: text_model.encoder.layers.5.self_attn.q_proj.weight: sum: 474.3468  dtype: Float32 shape: [1024,1024]
304: text_model.encoder.layers.5.self_attn.v_proj.bias: sum: 28.2114  dtype: Float32 shape: [1024]
305: text_model.encoder.layers.5.self_attn.v_proj.weight: sum: -719.4565  dtype: Float32 shape: [1024,1024]
306: text_model.encoder.layers.6.layer_norm1.bias: sum: 20.0706  dtype: Float32 shape: [1024]
307: text_model.encoder.layers.6.layer_norm1.weight: sum: 21.2805  dtype: Float32 shape: [1024]
308: text_model.encoder.layers.6.layer_norm2.bias: sum: -6.6203  dtype: Float32 shape: [1024]
309: text_model.encoder.layers.6.layer_norm2.weight: sum: 21.3578  dtype: Float32 shape: [1024]
310: text_model.encoder.layers.6.mlp.fc1.bias: sum: 42.6724  dtype: Float32 shape: [4096]
311: text_model.encoder.layers.6.mlp.fc1.weight: sum: 1397.1942  dtype: Float32 shape: [4096,1024]
312: text_model.encoder.layers.6.mlp.fc2.bias: sum: 10.1681  dtype: Float32 shape: [1024]
313: text_model.encoder.layers.6.mlp.fc2.weight: sum: 9262.755  dtype: Float32 shape: [1024,4096]
314: text_model.encoder.layers.6.self_attn.k_proj.bias: sum: 19.6665  dtype: Float32 shape: [1024]
315: text_model.encoder.layers.6.self_attn.k_proj.weight: sum: 2416.8857  dtype: Float32 shape: [1024,1024]
316: text_model.encoder.layers.6.self_attn.out_proj.bias: sum: -61.809  dtype: Float32 shape: [1024]
317: text_model.encoder.layers.6.self_attn.out_proj.weight: sum: 2285.8364  dtype: Float32 shape: [1024,1024]
318: text_model.encoder.layers.6.self_attn.q_proj.bias: sum: 38.2557  dtype: Float32 shape: [1024]
319: text_model.encoder.layers.6.self_attn.q_proj.weight: sum: 5169.315  dtype: Float32 shape: [1024,1024]
320: text_model.encoder.layers.6.self_attn.v_proj.bias: sum: 22.0628  dtype: Float32 shape: [1024]
321: text_model.encoder.layers.6.self_attn.v_proj.weight: sum: 268.9248  dtype: Float32 shape: [1024,1024]
322: text_model.encoder.layers.7.layer_norm1.bias: sum: 19.4622  dtype: Float32 shape: [1024]
323: text_model.encoder.layers.7.layer_norm1.weight: sum: 21.2821  dtype: Float32 shape: [1024]
324: text_model.encoder.layers.7.layer_norm2.bias: sum: 14.7301  dtype: Float32 shape: [1024]
325: text_model.encoder.layers.7.layer_norm2.weight: sum: 21.3514  dtype: Float32 shape: [1024]
326: text_model.encoder.layers.7.mlp.fc1.bias: sum: 42.6342  dtype: Float32 shape: [4096]
327: text_model.encoder.layers.7.mlp.fc1.weight: sum: 1311.8025  dtype: Float32 shape: [4096,1024]
328: text_model.encoder.layers.7.mlp.fc2.bias: sum: 43.6834  dtype: Float32 shape: [1024]
329: text_model.encoder.layers.7.mlp.fc2.weight: sum: 2573.7761  dtype: Float32 shape: [1024,4096]
330: text_model.encoder.layers.7.self_attn.k_proj.bias: sum: 15.2585  dtype: Float32 shape: [1024]
331: text_model.encoder.layers.7.self_attn.k_proj.weight: sum: 668.146  dtype: Float32 shape: [1024,1024]
332: text_model.encoder.layers.7.self_attn.out_proj.bias: sum: 40.6694  dtype: Float32 shape: [1024]
333: text_model.encoder.layers.7.self_attn.out_proj.weight: sum: 494.5207  dtype: Float32 shape: [1024,1024]
334: text_model.encoder.layers.7.self_attn.q_proj.bias: sum: 19.5563  dtype: Float32 shape: [1024]
335: text_model.encoder.layers.7.self_attn.q_proj.weight: sum: 620.3974  dtype: Float32 shape: [1024,1024]
336: text_model.encoder.layers.7.self_attn.v_proj.bias: sum: 16.1022  dtype: Float32 shape: [1024]
337: text_model.encoder.layers.7.self_attn.v_proj.weight: sum: 380.1792  dtype: Float32 shape: [1024,1024]
338: text_model.encoder.layers.8.layer_norm1.bias: sum: 19.6034  dtype: Float32 shape: [1024]
339: text_model.encoder.layers.8.layer_norm1.weight: sum: 21.3105  dtype: Float32 shape: [1024]
340: text_model.encoder.layers.8.layer_norm2.bias: sum: 15.2632  dtype: Float32 shape: [1024]
341: text_model.encoder.layers.8.layer_norm2.weight: sum: 21.3328  dtype: Float32 shape: [1024]
342: text_model.encoder.layers.8.mlp.fc1.bias: sum: 42.6978  dtype: Float32 shape: [4096]
343: text_model.encoder.layers.8.mlp.fc1.weight: sum: 1340.0182  dtype: Float32 shape: [4096,1024]
344: text_model.encoder.layers.8.mlp.fc2.bias: sum: 20.2457  dtype: Float32 shape: [1024]
345: text_model.encoder.layers.8.mlp.fc2.weight: sum: 2454.1433  dtype: Float32 shape: [1024,4096]
346: text_model.encoder.layers.8.self_attn.k_proj.bias: sum: 6.8926  dtype: Float32 shape: [1024]
347: text_model.encoder.layers.8.self_attn.k_proj.weight: sum: 952.9644  dtype: Float32 shape: [1024,1024]
348: text_model.encoder.layers.8.self_attn.out_proj.bias: sum: 1.939  dtype: Float32 shape: [1024]
349: text_model.encoder.layers.8.self_attn.out_proj.weight: sum: 3742.7463  dtype: Float32 shape: [1024,1024]
350: text_model.encoder.layers.8.self_attn.q_proj.bias: sum: 27.5315  dtype: Float32 shape: [1024]
351: text_model.encoder.layers.8.self_attn.q_proj.weight: sum: 1055.7065  dtype: Float32 shape: [1024,1024]
352: text_model.encoder.layers.8.self_attn.v_proj.bias: sum: 10.4081  dtype: Float32 shape: [1024]
353: text_model.encoder.layers.8.self_attn.v_proj.weight: sum: -1743.298  dtype: Float32 shape: [1024,1024]
354: text_model.encoder.layers.9.layer_norm1.bias: sum: 19.5283  dtype: Float32 shape: [1024]
355: text_model.encoder.layers.9.layer_norm1.weight: sum: 21.3081  dtype: Float32 shape: [1024]
356: text_model.encoder.layers.9.layer_norm2.bias: sum: 41.9961  dtype: Float32 shape: [1024]
357: text_model.encoder.layers.9.layer_norm2.weight: sum: 21.3279  dtype: Float32 shape: [1024]
358: text_model.encoder.layers.9.mlp.fc1.bias: sum: 42.6658  dtype: Float32 shape: [4096]
359: text_model.encoder.layers.9.mlp.fc1.weight: sum: 1355.9777  dtype: Float32 shape: [4096,1024]
360: text_model.encoder.layers.9.mlp.fc2.bias: sum: 28.805  dtype: Float32 shape: [1024]
361: text_model.encoder.layers.9.mlp.fc2.weight: sum: 3488.8645  dtype: Float32 shape: [1024,4096]
362: text_model.encoder.layers.9.self_attn.k_proj.bias: sum: 15.5856  dtype: Float32 shape: [1024]
363: text_model.encoder.layers.9.self_attn.k_proj.weight: sum: 10607.33  dtype: Float32 shape: [1024,1024]
364: text_model.encoder.layers.9.self_attn.out_proj.bias: sum: 54.5519  dtype: Float32 shape: [1024]
365: text_model.encoder.layers.9.self_attn.out_proj.weight: sum: -8398.903  dtype: Float32 shape: [1024,1024]
366: text_model.encoder.layers.9.self_attn.q_proj.bias: sum: 22.7645  dtype: Float32 shape: [1024]
367: text_model.encoder.layers.9.self_attn.q_proj.weight: sum: 685.2085  dtype: Float32 shape: [1024,1024]
368: text_model.encoder.layers.9.self_attn.v_proj.bias: sum: 33.9965  dtype: Float32 shape: [1024]
369: text_model.encoder.layers.9.self_attn.v_proj.weight: sum: 626.6072  dtype: Float32 shape: [1024,1024]
370: text_model.final_layer_norm.bias: sum: 21.487  dtype: Float32 shape: [1024]
371: text_model.final_layer_norm.weight: sum: 21.3191  dtype: Float32 shape: [1024]
